{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c30af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.161.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth-httplib2\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.24.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading proto_plus-1.26.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
      "Downloading google_api_python_client-2.161.0-py2.py3-none-any.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------  12.8/12.9 MB 89.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 42.5 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\n",
      "Downloading google_api_core-2.24.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading googleapis_common_protos-1.67.0-py2.py3-none-any.whl (164 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading proto_plus-1.26.0-py3-none-any.whl (50 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: uritemplate, rsa, proto-plus, oauthlib, httplib2, googleapis-common-protos, requests-oauthlib, google-auth, google-auth-oauthlib, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed google-api-core-2.24.1 google-api-python-client-2.161.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-auth-oauthlib-1.2.1 googleapis-common-protos-1.67.0 httplib2-0.22.0 oauthlib-3.2.2 proto-plus-1.26.0 requests-oauthlib-2.0.0 rsa-4.9 uritemplate-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b785a9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyodbc in c:\\users\\pbenko\\appdata\\local\\anaconda3\\lib\\site-packages (5.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856975ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File users.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# Define the scope for Google Drive API\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "# Function to authenticate and get Google Drive service\n",
    "def authenticate():\n",
    "    creds = None\n",
    "    token_file = 'token.json'\n",
    "\n",
    "    if os.path.exists(token_file):\n",
    "        creds = Credentials.from_authorized_user_file(token_file, SCOPES)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        with open(token_file, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    return creds\n",
    "\n",
    "# Function to upload a file to Google Drive\n",
    "def upload_to_drive(file_path, file_name):\n",
    "    creds = authenticate()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    file_metadata = {'name': file_name}\n",
    "    media = MediaFileUpload(file_path, mimetype='text/csv')\n",
    "\n",
    "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print(f\"File uploaded successfully. File ID: {file.get('id')}\")\n",
    "\n",
    "# Interactively specify the file to upload\n",
    "csv_file_path = 'users.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"Error: File {csv_file_path} does not exist.\")\n",
    "else:\n",
    "    upload_to_drive(csv_file_path, os.path.basename(csv_file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show google-auth\n",
    "!pip show google-auth-oauthlib\n",
    "!pip show google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61088b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-auth google-auth-oauthlib google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29500fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db652e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64197ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "output_csv = 'query_results.csv'\n",
    "credentials_file = 'credentials.json'\n",
    "\n",
    "# Establish the connection\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Write results to a CSV file\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(columns)  # Write column headers\n",
    "        writer.writerows(rows)  # Write data rows\n",
    "\n",
    "    print(f\"Results have been written to {output_csv}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n",
    "# Upload to Google Drive\n",
    "def upload_to_google_drive():\n",
    "    # Authenticate and create the Drive API service\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(credentials_file, SCOPES)\n",
    "    creds = flow.run_local_server(port=0)\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Folder ID from the provided link\n",
    "    folder_id = '1OqOktV99WyRrloXigw-UhxA7DuEQ8NFf'\n",
    "\n",
    "    # Upload the file to the specified folder\n",
    "    file_metadata = {\n",
    "        'name': output_csv,\n",
    "        'parents': [folder_id]  # Specify the parent folder\n",
    "    }\n",
    "    media = MediaFileUpload(output_csv, mimetype='text/csv')\n",
    "    uploaded_file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "    print(f\"File uploaded to Google Drive folder with ID: {uploaded_file.get('id')}\")\n",
    "\n",
    "\n",
    "# Call the upload function\n",
    "try:\n",
    "    upload_to_google_drive()\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to Google Drive: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b781b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "credentials_file = 'credentials.json'\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.test2\"\n",
    "\n",
    "# Establish the connection\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Write results to a CSV file\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(columns)  # Write column headers\n",
    "        writer.writerows(rows)  # Write data rows\n",
    "\n",
    "    print(f\"Results have been written to {output_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0c9b1",
   "metadata": {},
   "source": [
    "### Query SQL Server to get Divrt Stations Metadata and Upload them to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a10231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQL Server was successful.\n",
      "Query is done.\n",
      "Loaded 766 rows and 23 columns to ut-gee-wri-hydro-dev.test.Things\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery  \n",
    "from google.oauth2 import service_account  \n",
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "credentials_file = 'credentials.json'\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.Things\"\n",
    "\n",
    "\n",
    "# Path to your service account key\n",
    "credentialsPath = r'ut-gee-wri-hydro-dev-e91fd0400fd0.json'  \n",
    "credentials = service_account.Credentials.from_service_account_file(credentialsPath)  \n",
    "client = bigquery.Client(credentials=credentials)  \n",
    "\n",
    "# Define the table ID and file path\n",
    "\n",
    "\n",
    "# Establish the connection to SQL Server\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()  # Fetch all rows\n",
    "\n",
    "    # Prepare data for BigQuery\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Define BigQuery job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite existing data\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON  # Use JSON format for rows\n",
    "    )\n",
    "\n",
    "    print(\"Query is done.\")\n",
    "    \n",
    "    # Load data to BigQuery\n",
    "    job = client.load_table_from_json(data, table_id, job_config=job_config)  # Load data directly from memory\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "\n",
    "    # Confirm the load\n",
    "    table = client.get_table(table_id)  # Make an API request\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query or loading data to BigQuery: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd056b70",
   "metadata": {},
   "source": [
    "### Query SQL Server to get Divrt Stations Data Streams Upload them to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery  \n",
    "from google.oauth2 import service_account  \n",
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "credentials_file = 'credentials.json'\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.Datastreams\"\n",
    "\n",
    "\n",
    "# Path to your service account key\n",
    "credentialsPath = r'ut-gee-wri-hydro-dev-e91fd0400fd0.json'  \n",
    "credentials = service_account.Credentials.from_service_account_file(credentialsPath)  \n",
    "client = bigquery.Client(credentials=credentials)  \n",
    "\n",
    "# Define the table ID and file path\n",
    "\n",
    "\n",
    "# Establish the connection to SQL Server\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()  # Fetch all rows\n",
    "\n",
    "    # Prepare data for BigQuery\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Define BigQuery job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite existing data\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON  # Use JSON format for rows\n",
    "    )\n",
    "\n",
    "    print(\"Query is done.\")\n",
    "    \n",
    "    # Load data to BigQuery\n",
    "    job = client.load_table_from_json(data, table_id, job_config=job_config)  # Load data directly from memory\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "\n",
    "    # Confirm the load\n",
    "    table = client.get_table(table_id)  # Make an API request\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query or loading data to BigQuery: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419be439",
   "metadata": {},
   "source": [
    "### Query the Daily Records table and Upload it to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Define connection parameters for the SQL Server\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'DAILY_RECORDS.sql'\n",
    "\n",
    "# BigQuery configuration\n",
    "credentialsPath = r'ut-gee-wri-hydro-dev-e91fd0400fd0.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(credentialsPath)\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.DAILY_RECORDS\"\n",
    "\n",
    "# Establish the connection to SQL Server\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()  # Fetch all rows\n",
    "\n",
    "    # Prepare data for BigQuery\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Define BigQuery job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite existing data\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON  # Use JSON format for rows\n",
    "    )\n",
    "\n",
    "    print(\"Query is done.\")\n",
    "    \n",
    "    # Load data to BigQuery\n",
    "    job = client.load_table_from_json(data, table_id, job_config=job_config)  # Load data directly from memory\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "\n",
    "    # Confirm the load\n",
    "    table = client.get_table(table_id)  # Make an API request\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query or loading data to BigQuery: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e99108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
