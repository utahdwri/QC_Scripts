{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c30af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b785a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856975ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "# Define the scope for Google Drive API\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "# Function to authenticate and get Google Drive service\n",
    "def authenticate():\n",
    "    creds = None\n",
    "    token_file = 'token.json'\n",
    "\n",
    "    if os.path.exists(token_file):\n",
    "        creds = Credentials.from_authorized_user_file(token_file, SCOPES)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "            creds = flow.run_local_server(port=0)\n",
    "\n",
    "        with open(token_file, 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "\n",
    "    return creds\n",
    "\n",
    "# Function to upload a file to Google Drive\n",
    "def upload_to_drive(file_path, file_name):\n",
    "    creds = authenticate()\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    file_metadata = {'name': file_name}\n",
    "    media = MediaFileUpload(file_path, mimetype='text/csv')\n",
    "\n",
    "    file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "    print(f\"File uploaded successfully. File ID: {file.get('id')}\")\n",
    "\n",
    "# Interactively specify the file to upload\n",
    "csv_file_path = 'users.csv'\n",
    "if not os.path.exists(csv_file_path):\n",
    "    print(f\"Error: File {csv_file_path} does not exist.\")\n",
    "else:\n",
    "    upload_to_drive(csv_file_path, os.path.basename(csv_file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962b16a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show google-auth\n",
    "!pip show google-auth-oauthlib\n",
    "!pip show google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61088b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-auth google-auth-oauthlib google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdc68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29500fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db652e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64197ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "output_csv = 'query_results.csv'\n",
    "credentials_file = 'credentials.json'\n",
    "\n",
    "# Establish the connection\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Write results to a CSV file\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(columns)  # Write column headers\n",
    "        writer.writerows(rows)  # Write data rows\n",
    "\n",
    "    print(f\"Results have been written to {output_csv}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n",
    "# Upload to Google Drive\n",
    "def upload_to_google_drive():\n",
    "    # Authenticate and create the Drive API service\n",
    "    SCOPES = ['https://www.googleapis.com/auth/drive.file']\n",
    "\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(credentials_file, SCOPES)\n",
    "    creds = flow.run_local_server(port=0)\n",
    "    service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "    # Folder ID from the provided link\n",
    "    folder_id = '1OqOktV99WyRrloXigw-UhxA7DuEQ8NFf'\n",
    "\n",
    "    # Upload the file to the specified folder\n",
    "    file_metadata = {\n",
    "        'name': output_csv,\n",
    "        'parents': [folder_id]  # Specify the parent folder\n",
    "    }\n",
    "    media = MediaFileUpload(output_csv, mimetype='text/csv')\n",
    "    uploaded_file = service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "    print(f\"File uploaded to Google Drive folder with ID: {uploaded_file.get('id')}\")\n",
    "\n",
    "\n",
    "# Call the upload function\n",
    "try:\n",
    "    upload_to_google_drive()\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading to Google Drive: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-bigquery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b781b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "credentials_file = 'credentials.json'\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.test2\"\n",
    "\n",
    "# Establish the connection\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Write results to a CSV file\n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(columns)  # Write column headers\n",
    "        writer.writerows(rows)  # Write data rows\n",
    "\n",
    "    print(f\"Results have been written to {output_csv}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0c9b1",
   "metadata": {},
   "source": [
    "### Query SQL Server to get Divrt Stations Metadata and Upload them to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a10231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to SQL Server was successful.\n",
      "Query is done.\n",
      "Loaded 766 rows and 23 columns to ut-gee-wri-hydro-dev.test.Things\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery  \n",
    "from google.oauth2 import service_account  \n",
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "credentials_file = 'credentials.json'\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.Things\"\n",
    "\n",
    "\n",
    "# Path to your service account key\n",
    "credentialsPath = r'ut-gee-wri-hydro-dev-e91fd0400fd0.json'  \n",
    "credentials = service_account.Credentials.from_service_account_file(credentialsPath)  \n",
    "client = bigquery.Client(credentials=credentials)  \n",
    "\n",
    "# Define the table ID and file path\n",
    "\n",
    "\n",
    "# Establish the connection to SQL Server\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()  # Fetch all rows\n",
    "\n",
    "    # Prepare data for BigQuery\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Define BigQuery job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite existing data\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON  # Use JSON format for rows\n",
    "    )\n",
    "\n",
    "    print(\"Query is done.\")\n",
    "    \n",
    "    # Load data to BigQuery\n",
    "    job = client.load_table_from_json(data, table_id, job_config=job_config)  # Load data directly from memory\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "\n",
    "    # Confirm the load\n",
    "    table = client.get_table(table_id)  # Make an API request\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query or loading data to BigQuery: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd056b70",
   "metadata": {},
   "source": [
    "### Query SQL Server to get Divrt Stations Data Streams Upload them to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edf9f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery  \n",
    "from google.oauth2 import service_account  \n",
    "import pyodbc\n",
    "import csv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import os\n",
    "\n",
    "# Define connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'test.sql'\n",
    "credentials_file = 'credentials.json'\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.Datastreams\"\n",
    "\n",
    "\n",
    "# Path to your service account key\n",
    "credentialsPath = r'ut-gee-wri-hydro-dev-e91fd0400fd0.json'  \n",
    "credentials = service_account.Credentials.from_service_account_file(credentialsPath)  \n",
    "client = bigquery.Client(credentials=credentials)  \n",
    "\n",
    "# Define the table ID and file path\n",
    "\n",
    "\n",
    "# Establish the connection to SQL Server\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()  # Fetch all rows\n",
    "\n",
    "    # Prepare data for BigQuery\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Define BigQuery job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite existing data\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON  # Use JSON format for rows\n",
    "    )\n",
    "\n",
    "    print(\"Query is done.\")\n",
    "    \n",
    "    # Load data to BigQuery\n",
    "    job = client.load_table_from_json(data, table_id, job_config=job_config)  # Load data directly from memory\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "\n",
    "    # Confirm the load\n",
    "    table = client.get_table(table_id)  # Make an API request\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query or loading data to BigQuery: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419be439",
   "metadata": {},
   "source": [
    "### Query the Daily Records table and Upload it to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b27a965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Define connection parameters for the SQL Server\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "sql_file = 'DAILY_RECORDS.sql'\n",
    "\n",
    "# BigQuery configuration\n",
    "credentialsPath = r'ut-gee-wri-hydro-dev-e91fd0400fd0.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(credentialsPath)\n",
    "client = bigquery.Client(credentials=credentials)\n",
    "table_id = \"ut-gee-wri-hydro-dev.test.DAILY_RECORDS\"\n",
    "\n",
    "# Establish the connection to SQL Server\n",
    "try:\n",
    "    conn = pyodbc.connect(\n",
    "        f'DRIVER={{ODBC Driver 17 for SQL Server}};'\n",
    "        f'SERVER={server};'\n",
    "        f'DATABASE={database};'\n",
    "        f'UID={username};'\n",
    "        f'PWD={password}'\n",
    "    )\n",
    "    print(\"Connection to SQL Server was successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to SQL Server: {e}\")\n",
    "    raise\n",
    "\n",
    "# Read and execute the SQL query\n",
    "try:\n",
    "    with open(sql_file, 'r') as file:\n",
    "        query = file.read()\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch results\n",
    "    columns = [column[0] for column in cursor.description]  # Get column names\n",
    "    rows = cursor.fetchall()  # Fetch all rows\n",
    "\n",
    "    # Prepare data for BigQuery\n",
    "    data = [dict(zip(columns, row)) for row in rows]\n",
    "    \n",
    "    # Define BigQuery job configuration\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # Overwrite existing data\n",
    "        source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON  # Use JSON format for rows\n",
    "    )\n",
    "\n",
    "    print(\"Query is done.\")\n",
    "    \n",
    "    # Load data to BigQuery\n",
    "    job = client.load_table_from_json(data, table_id, job_config=job_config)  # Load data directly from memory\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    job.result()\n",
    "\n",
    "    # Confirm the load\n",
    "    table = client.get_table(table_id)  # Make an API request\n",
    "    print(\"Loaded {} rows and {} columns to {}\".format(table.num_rows, len(table.schema), table_id))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing query or loading data to BigQuery: {e}\")\n",
    "finally:\n",
    "    # Clean up and close the connection\n",
    "    conn.close()\n",
    "    print(\"Connection closed.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e99108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
