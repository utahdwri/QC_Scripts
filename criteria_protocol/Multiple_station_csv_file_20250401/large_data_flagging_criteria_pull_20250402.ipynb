{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c55f91-931a-419f-a16c-ed9b0e5a042b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged data saved as large_merged_60_station_metadata_results.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import time\n",
    "\n",
    "time.sleep(1)  # Wait 1 second between requests\n",
    "\n",
    "# Load metadata CSV (ensure STATION_ID is a string)\n",
    "metadata_path = \"C:\\\\Users\\\\pbenko\\\\Documents\\\\SQL saved scripts\\\\meta_data_for_all_active_systems_for_calculation_20250401(CSV).csv\"\n",
    "metadata_df = pd.read_csv(metadata_path, dtype={\"STATION_ID\": str})\n",
    "\n",
    "# Extract first 60 rows of metadata\n",
    "first_60_metadata = metadata_df.head(60)\n",
    "\n",
    "# Extract unique STATION_IDs from the first 60 rows\n",
    "site_ids = first_60_metadata[\"STATION_ID\"].dropna().unique().tolist()\n",
    "\n",
    "# Filter metadata based on these STATION_IDs\n",
    "filtered_metadata = metadata_df[metadata_df[\"STATION_ID\"].isin(site_ids)].copy()\n",
    "\n",
    "# Initialize results DataFrame\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "# Loop through each STATION_ID from the first 5 rows\n",
    "for DivrtID in site_ids:\n",
    "    end_date = datetime.today().strftime(\"%Y-%m-%d\")  # Get today's date in YYYY-MM-DD format\n",
    "    api_url = f\"https://www.waterrights.utah.gov/dvrtdb/daily-chart.asp?station_id={DivrtID}&end_date={end_date}&f=json\"\n",
    "\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"data\" in data:\n",
    "            df = pd.DataFrame(data[\"data\"], columns=[\"date\", \"value\"])\n",
    "            df.rename(columns={\"date\": \"Date\", \"value\": \"DISCHARGE\"}, inplace=True)\n",
    "            df[\"STATION_ID\"] = DivrtID  # Ensure STATION_ID is included\n",
    "\n",
    "            # Convert 'DISCHARGE' to numeric\n",
    "            df[\"DISCHARGE\"] = pd.to_numeric(df[\"DISCHARGE\"], errors=\"coerce\")\n",
    "\n",
    "            # === FLAGGING CRITERIA === #\n",
    "            df[\"FLAG_NEGATIVE\"] = df[\"DISCHARGE\"] < 0\n",
    "            df[\"FLAG_ZERO\"] = df[\"DISCHARGE\"] == 0\n",
    "\n",
    "            # Filter out zero values before statistical computations\n",
    "            df_nonzero = df[df[\"DISCHARGE\"] > 0].copy()\n",
    "\n",
    "            if not df_nonzero.empty:\n",
    "                # Compute 95th percentile **excluding zero values**\n",
    "                discharge_95th_percentile = np.percentile(df_nonzero[\"DISCHARGE\"].dropna(), 95)\n",
    "\n",
    "                # Compute IQR **excluding zero values**\n",
    "                Q1, Q3 = df_nonzero[\"DISCHARGE\"].quantile([0.25, 0.75])\n",
    "                IQR = Q3 - Q1\n",
    "\n",
    "                # Compute rate of change **excluding zero values**\n",
    "                df_nonzero[\"RATE_OF_CHANGE\"] = df_nonzero[\"DISCHARGE\"].diff().abs()\n",
    "                df = df.merge(df_nonzero[[\"Date\", \"RATE_OF_CHANGE\"]], on=\"Date\", how=\"left\")\n",
    "\n",
    "                # Compute repeated values **excluding zero values**\n",
    "                df_nonzero[\"FLAG_REPEATED\"] = df_nonzero[\"DISCHARGE\"].groupby(\n",
    "                    (df_nonzero[\"DISCHARGE\"] != df_nonzero[\"DISCHARGE\"].shift()).cumsum()\n",
    "                ).transform(\"count\") >= 3\n",
    "\n",
    "                # Apply Isolation Forest **excluding zero values**\n",
    "                model = IsolationForest(contamination=0.05, random_state=42)\n",
    "                df_nonzero[\"OUTLIER_IF\"] = model.fit_predict(df_nonzero[[\"DISCHARGE\"]])\n",
    "                df_nonzero[\"OUTLIER_IF\"] = df_nonzero[\"OUTLIER_IF\"] == -1  # Convert to boolean\n",
    "\n",
    "                # Merge non-zero flags back into main DataFrame\n",
    "                df = df.merge(df_nonzero[[\"Date\", \"OUTLIER_IF\", \"FLAG_REPEATED\"]], on=\"Date\", how=\"left\")\n",
    "            else:\n",
    "                discharge_95th_percentile = 0\n",
    "                IQR = 0\n",
    "                df[\"RATE_OF_CHANGE\"] = np.nan\n",
    "                df[\"OUTLIER_IF\"] = False\n",
    "                df[\"FLAG_REPEATED\"] = False\n",
    "\n",
    "            # Apply flagging based on computed values\n",
    "            df[\"FLAG_Discharge\"] = df[\"DISCHARGE\"] > discharge_95th_percentile\n",
    "            df[\"FLAG_IQR\"] = (df[\"DISCHARGE\"] < Q1 - 1.5 * IQR) | (df[\"DISCHARGE\"] > Q3 + 1.5 * IQR)\n",
    "            df[\"FLAG_RoC\"] = df[\"RATE_OF_CHANGE\"] > discharge_95th_percentile\n",
    "\n",
    "            df[\"FLAGGED\"] = df[\n",
    "                [\"FLAG_NEGATIVE\", \"FLAG_ZERO\", \"FLAG_REPEATED\", \"FLAG_IQR\", \"OUTLIER_IF\", \"FLAG_Discharge\", \"FLAG_RoC\"]\n",
    "            ].any(axis=1)\n",
    "\n",
    "            # Create summary for the current station\n",
    "            station_summary = {\n",
    "                \"STATION_ID\": DivrtID,\n",
    "                \"TOTAL_RECORDS\": len(df),\n",
    "                \"TOTAL_NEGATIVE\": df[\"FLAG_NEGATIVE\"].sum(),\n",
    "                \"TOTAL_ZERO\": df[\"FLAG_ZERO\"].sum(),\n",
    "                \"TOTAL_95th\": df[\"FLAG_Discharge\"].sum(),\n",
    "                \"TOTAL_IQR\": df[\"FLAG_IQR\"].sum(),\n",
    "                \"TOTAL_RoC\": df[\"FLAG_RoC\"].sum(),\n",
    "                \"TOTAL_REPEATED\": df[\"FLAG_REPEATED\"].sum(),\n",
    "                \"TOTAL_IF\": df[\"OUTLIER_IF\"].sum(),\n",
    "                \"TOTAL_FLAGGED\": df[\"FLAGGED\"].sum(),\n",
    "                \"FLAG_RATIO\": f\"{(df['FLAGGED'].sum() / len(df) * 100):.2f}%\" if len(df) > 0 else \"0.00%\",\n",
    "                \"ZERO_RATIO\": f\"{(df['FLAG_ZERO'].sum() / len(df) * 100):.2f}%\" if len(df) > 0 else \"0.00%\"\n",
    "            }\n",
    "\n",
    "            # Append the summary to results\n",
    "            df_results = pd.concat([df_results, pd.DataFrame([station_summary])], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Error: 'data' key not found for STATION_ID {DivrtID}\")\n",
    "    else:\n",
    "        print(f\"Error fetching data for STATION_ID {DivrtID}: {response.status_code}\")\n",
    "\n",
    "# Merge metadata with results on STATION_ID\n",
    "merged_df = pd.merge(filtered_metadata, df_results, on=\"STATION_ID\", how=\"left\")\n",
    "\n",
    "# Save the final merged CSV\n",
    "output_filename = \"large_merged_60_station_metadata_results.csv\"\n",
    "merged_df.to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"Final merged data saved as {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a8cf3-e3e2-4a78-85ac-be04daaa0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "# Load metadata CSV\n",
    "metadata_path = \"C:\\\\Users\\\\pbenko\\\\Documents\\\\SQL saved scripts\\\\meta_data_for_all_active_systems_for_calculation_20250401(CSV).csv\"\n",
    "metadata_df = pd.read_csv(metadata_path, dtype={\"STATION_ID\": str})  # Ensure STATION_ID is a string\n",
    "\n",
    "# Extract first 60 rows of metadata\n",
    "first_60_metadata = metadata_df.head(60)\n",
    "\n",
    "# Extract unique STATION_IDs from the first 60 rows\n",
    "site_ids = first_60_metadata[\"STATION_ID\"].dropna().unique().tolist()\n",
    "\n",
    "# Filter metadata based on these STATION_IDs\n",
    "filtered_metadata = metadata_df[metadata_df[\"STATION_ID\"].isin(site_ids)].copy()\n",
    "\n",
    "# Initialize results DataFrame\n",
    "df_results = pd.DataFrame()\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Loop through each STATION_ID with a progress bar\n",
    "for i, DivrtID in enumerate(tqdm(site_ids, desc=\"Processing Stations\", unit=\"station\")):\n",
    "    loop_start = time.time()  # Track loop timing\n",
    "    end_date = datetime.today().strftime(\"%Y-%m-%d\")  # Get today's date in YYYY-MM-DD format\n",
    "    api_url = f\"https://www.waterrights.utah.gov/dvrtdb/daily-chart.asp?station_id={DivrtID}&end_date={end_date}&f=json\"\n",
    "\n",
    "    response = requests.get(api_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if \"data\" in data:\n",
    "            df = pd.DataFrame(data[\"data\"], columns=[\"date\", \"value\"])\n",
    "            df.rename(columns={\"date\": \"Date\", \"value\": \"DISCHARGE\"}, inplace=True)\n",
    "            df[\"STATION_ID\"] = DivrtID  # Ensure STATION_ID is included\n",
    "\n",
    "            # Convert 'DISCHARGE' to numeric\n",
    "            df[\"DISCHARGE\"] = pd.to_numeric(df[\"DISCHARGE\"], errors=\"coerce\")\n",
    "\n",
    "            # === FLAGGING CRITERIA === #\n",
    "            df[\"FLAG_NEGATIVE\"] = df[\"DISCHARGE\"] < 0\n",
    "            df[\"FLAG_ZERO\"] = df[\"DISCHARGE\"] == 0\n",
    "\n",
    "            # Filter out zero values before statistical computations\n",
    "            df_nonzero = df[df[\"DISCHARGE\"] > 0].copy()\n",
    "\n",
    "            if not df_nonzero.empty:\n",
    "                # Compute 95th percentile **excluding zero values**\n",
    "                discharge_95th_percentile = np.percentile(df_nonzero[\"DISCHARGE\"].dropna(), 95)\n",
    "                \n",
    "                # Compute IQR **excluding zero values**\n",
    "                Q1, Q3 = df_nonzero[\"DISCHARGE\"].quantile([0.25, 0.75])\n",
    "                IQR = Q3 - Q1\n",
    "\n",
    "                # Compute rate of change **excluding zero values**\n",
    "                df_nonzero[\"RATE_OF_CHANGE\"] = df_nonzero[\"DISCHARGE\"].diff().abs()\n",
    "                df = df.merge(df_nonzero[[\"Date\", \"RATE_OF_CHANGE\"]], on=\"Date\", how=\"left\")\n",
    "\n",
    "                # Compute repeated values **excluding zero values**\n",
    "                df_nonzero[\"FLAG_REPEATED\"] = df_nonzero[\"DISCHARGE\"].groupby(\n",
    "                    (df_nonzero[\"DISCHARGE\"] != df_nonzero[\"DISCHARGE\"].shift()).cumsum()\n",
    "                ).transform(\"count\") >= 3\n",
    "\n",
    "                # Apply Isolation Forest **excluding zero values**\n",
    "                model = IsolationForest(contamination=0.05, random_state=42)\n",
    "                df_nonzero[\"OUTLIER_IF\"] = model.fit_predict(df_nonzero[[\"DISCHARGE\"]])\n",
    "                df_nonzero[\"OUTLIER_IF\"] = df_nonzero[\"OUTLIER_IF\"] == -1  # Convert to boolean\n",
    "                \n",
    "                # Merge non-zero flags back into main DataFrame\n",
    "                df = df.merge(df_nonzero[[\"Date\", \"OUTLIER_IF\", \"FLAG_REPEATED\"]], on=\"Date\", how=\"left\")\n",
    "            else:\n",
    "                discharge_95th_percentile = 0\n",
    "                IQR = 0\n",
    "                df[\"RATE_OF_CHANGE\"] = np.nan\n",
    "                df[\"OUTLIER_IF\"] = False\n",
    "                df[\"FLAG_REPEATED\"] = False\n",
    "\n",
    "            # Apply flagging based on computed values\n",
    "            df[\"FLAG_Discharge\"] = df[\"DISCHARGE\"] > discharge_95th_percentile\n",
    "            df[\"FLAG_IQR\"] = (df[\"DISCHARGE\"] < Q1 - 1.5 * IQR) | (df[\"DISCHARGE\"] > Q3 + 1.5 * IQR)\n",
    "            df[\"FLAG_RoC\"] = df[\"RATE_OF_CHANGE\"] > discharge_95th_percentile\n",
    "\n",
    "            df[\"FLAGGED\"] = df[\n",
    "                [\"FLAG_NEGATIVE\", \"FLAG_ZERO\", \"FLAG_REPEATED\", \"FLAG_IQR\", \"OUTLIER_IF\", \"FLAG_Discharge\", \"FLAG_RoC\"]\n",
    "            ].any(axis=1)\n",
    "\n",
    "            # Create summary for the current station\n",
    "            station_summary = {\n",
    "                \"STATION_ID\": DivrtID,\n",
    "                \"TOTAL_RECORDS\": len(df),\n",
    "                \"TOTAL_NEGATIVE\": df[\"FLAG_NEGATIVE\"].sum(),\n",
    "                \"TOTAL_ZERO\": df[\"FLAG_ZERO\"].sum(),\n",
    "                \"TOTAL_95th\": df[\"FLAG_Discharge\"].sum(),\n",
    "                \"TOTAL_IQR\": df[\"FLAG_IQR\"].sum(),\n",
    "                \"TOTAL_RoC\": df[\"FLAG_RoC\"].sum(),\n",
    "                \"TOTAL_REPEATED\": df[\"FLAG_REPEATED\"].sum(),\n",
    "                \"TOTAL_IF\": df[\"OUTLIER_IF\"].sum(),\n",
    "                \"TOTAL_FLAGGED\": df[\"FLAGGED\"].sum(),\n",
    "                \"FLAG_RATIO\": f\"{(df['FLAGGED'].sum() / len(df) * 100):.2f}%\" if len(df) > 0 else \"0.00%\",\n",
    "                \"ZERO_RATIO\": f\"{(df['FLAG_ZERO'].sum() / len(df) * 100):.2f}%\" if len(df) > 0 else \"0.00%\"\n",
    "            }\n",
    "\n",
    "            # Append the summary to results\n",
    "            df_results = pd.concat([df_results, pd.DataFrame([station_summary])], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Error: 'data' key not found for STATION_ID {DivrtID}\")\n",
    "    else:\n",
    "        print(f\"Error fetching data for STATION_ID {DivrtID}: {response.status_code}\")\n",
    "\n",
    "    # Estimate time remaining\n",
    "    elapsed_time = time.time() - loop_start\n",
    "    remaining_time = elapsed_time * (len(site_ids) - (i + 1))\n",
    "    print(f\"Estimated time remaining: {remaining_time / 60:.2f} minutes\")\n",
    "\n",
    "# Merge metadata with results on STATION_ID\n",
    "merged_df = pd.merge(filtered_metadata, df_results, on=\"STATION_ID\", how=\"left\")\n",
    "\n",
    "# Save the final merged CSV\n",
    "output_filename = \"merged_60_station_metadata_results.csv\"\n",
    "merged_df.to_csv(output_filename, index=False)\n",
    "\n",
    "# Total time taken\n",
    "total_time = (time.time() - start_time) / 60\n",
    "print(f\"Final merged data saved as {output_filename}\")\n",
    "print(f\"Total execution time: {total_time:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
