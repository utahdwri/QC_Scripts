{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3dd48dd-cfb6-4332-b4d3-e5711f37c31d",
   "metadata": {},
   "source": [
    "### Code to get all reservior data in the ouput formate requested by USU with collection system\n",
    "### this runs for the DataStreams then the following code takes the output csv file and \n",
    "### creates the site formated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adae88b-67c4-4a6a-823f-ff37c607f5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# --- SQL SERVER CONNECTION SETUP ---\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = '********'\n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    ")\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# --- STEP 1: SQL METADATA QUERY ---\n",
    "def query_station_metadata():\n",
    "    sql_query = \"\"\"\n",
    "    SELECT  \n",
    "        LTRIM(RTRIM([COLLECTION_SYSTEM])) AS COLLECTION_SYSTEM,\n",
    "        LTRIM(RTRIM([collection_sys_description])) AS collection_sys_description,\n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        LTRIM(RTRIM([STATION_MASTER].[STATION_NAME])) AS MasterStationName,\n",
    "        LTRIM(RTRIM([COLLECTION_STATIONS].[STATION_NAME])) AS CollectionStationName,\n",
    "        LTRIM(RTRIM([COMMENTS])) AS COMMENTS,\n",
    "        LTRIM(RTRIM([SiteType])) AS SiteType,\n",
    "        LTRIM(RTRIM([ANALOG_CHANNEL])) AS ANALOG_CHANNEL,\n",
    "        LTRIM(RTRIM([SYSTEM_NAME])) AS SYSTEM_NAME,\n",
    "        LTRIM(RTRIM([DatasetType])) AS DatasetType,\n",
    "        LTRIM(RTRIM([MEASURING_DEVICE])) AS MEASURING_DEVICE,\n",
    "        LTRIM(RTRIM([DEVICE_TYPE])) AS DEVICE_TYPE,\n",
    "        LTRIM(RTRIM([STATUS])) AS STATUS,\n",
    "        [LAT], [LON],\n",
    "        LTRIM(RTRIM([DataEntryMethod])) AS DataEntryMethod,\n",
    "        LTRIM(RTRIM([Telemetry])) AS Telemetry,\n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', [STATION_MASTER].[STATION_ID]) AS StationPage,\n",
    "        [UNITS_MASTER].[UNITS_ID],\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[RECORD_TYPE])) AS RECORD_TYPE,\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_BASE])) AS UNITS_DESC_BASE,\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_ENTRY])) AS UNITS_DESC_ENTRY,\n",
    "        [UNITS_MASTER].[UNITS_MULTIPLIER],\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_REALTIME])) AS UNITS_DESC_REALTIME,\n",
    "        COUNT([RECORD_YEAR]) AS NoOfYears, \n",
    "        MIN([RECORD_YEAR]) AS StartYr, \n",
    "        MAX([RECORD_YEAR]) AS EndYr\n",
    "    FROM [dvrtDB].[dbo].[STATION_MASTER]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] \n",
    "        ON [COLLECTION_SYSTEMS].[collection_sys_id] = [STATION_MASTER].[STATION_ID]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_STATIONS] \n",
    "        ON [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] \n",
    "        ON [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[DAILY_RECORDS] \n",
    "        ON [STATION_MASTER].[STATION_ID] = [DAILY_RECORDS].[STATION_ID]\n",
    "    WHERE \n",
    "        [STATUS] = 'A' AND\n",
    "        [DatasetType] = 'Observational' AND\n",
    "        [DataEntryMethod] != 'Manual' AND\n",
    "        [DataEntryMethod] IS NOT NULL AND\n",
    "        ([LAT] IS NOT NULL OR [LON] IS NOT NULL) AND\n",
    "        ([LON] > '-115' OR [LON] < '36') AND\n",
    "        [LAT] > 0 AND\n",
    "        (\n",
    "            [STATION_MASTER].[STATION_NAME] LIKE '%Reservoir%' OR \n",
    "            [COLLECTION_STATIONS].[STATION_NAME] LIKE '%Reservoir%'\n",
    "        )\n",
    "    GROUP BY\n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], [STATION_MASTER].[STATION_ID],\n",
    "        [STATION_MASTER].[STATION_NAME], [COLLECTION_STATIONS].[STATION_NAME],\n",
    "        [COMMENTS], [SiteType], [ANALOG_CHANNEL], [SYSTEM_NAME], [DatasetType],\n",
    "        [MEASURING_DEVICE], [DEVICE_TYPE], [STATUS], [LAT], [LON],\n",
    "        [DataEntryMethod], [Telemetry],\n",
    "        [UNITS_MASTER].[UNITS_ID], [UNITS_MASTER].[RECORD_TYPE],\n",
    "        [UNITS_MASTER].[UNITS_DESC_BASE], [UNITS_MASTER].[UNITS_DESC_ENTRY],\n",
    "        [UNITS_MASTER].[UNITS_MULTIPLIER], [UNITS_MASTER].[UNITS_DESC_REALTIME]\n",
    "    ORDER BY [STATION_MASTER].[STATION_ID]\n",
    "    \"\"\"\n",
    "    return pd.read_sql(sql_query, engine)\n",
    "\n",
    "# --- STEP 2: FINAL EXPORT LOGIC WITH SPLIT SITE ID ---\n",
    "def generate_final_export(df):\n",
    "    df = df[df[\"SiteType\"].isin([\"Reservoir\", \"Reservoir Release\"])].copy()\n",
    "\n",
    "    # Tag which rows are release-type\n",
    "    df[\"IsRelease\"] = df[\"MasterStationName\"].str.upper().str.contains(\"RELEASE\")\n",
    "\n",
    "    # Extract base name (remove USBR etc.)\n",
    "    def extract_base_name(name):\n",
    "        name = str(name).upper()\n",
    "        name = re.sub(r'\\b(USBR|USGS|RESERVOIR|RELEASE|CONTENTS|ELEVATION|STORAGE|POOL|EVAPORATION|LEVEL|GAGE HEIGHT)\\b', '', name)\n",
    "        name = re.sub(r'[^A-Z0-9 ]+', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name.split()[0].title() if name else \"Unknown\"\n",
    "\n",
    "    df[\"ReservoirRootName\"] = df[\"MasterStationName\"].apply(extract_base_name)\n",
    "\n",
    "    # === Split ===\n",
    "    release_df = df[df[\"IsRelease\"]].copy()\n",
    "    nonrelease_df = df[~df[\"IsRelease\"]].copy()\n",
    "\n",
    "    # Assign PD### to non-release groups by root name\n",
    "    unique_nonreleases = nonrelease_df[\"ReservoirRootName\"].unique()\n",
    "    nonrelease_group_map = {name: f\"PD{str(i+1).zfill(3)}\" for i, name in enumerate(unique_nonreleases)}\n",
    "    nonrelease_df[\"SiteID (New)\"] = nonrelease_df[\"ReservoirRootName\"].map(nonrelease_group_map)\n",
    "\n",
    "    # Assign PD### to each release row independently\n",
    "    release_df = release_df.reset_index(drop=True)\n",
    "    release_df[\"SiteID (New)\"] = [\"PD\" + str(i + len(nonrelease_group_map) + 1).zfill(3) for i in range(len(release_df))]\n",
    "\n",
    "    # Merge back\n",
    "    final_df = pd.concat([nonrelease_df, release_df], ignore_index=True)\n",
    "\n",
    "    # Build NewSiteName\n",
    "    def make_site_name(row):\n",
    "        base = row[\"ReservoirRootName\"]\n",
    "        system = row[\"SYSTEM_NAME\"].strip().title()\n",
    "        if row[\"IsRelease\"]:\n",
    "            return f\"{base} River Below {base} Reservoir, {system}, Near {base}\"\n",
    "        else:\n",
    "            return f\"{base} Reservoir, {system}, Near {base}\"\n",
    "\n",
    "    final_df[\"NewSiteName\"] = final_df.apply(make_site_name, axis=1)\n",
    "\n",
    "    # Sort and assign DS##\n",
    "    final_df = final_df.sort_values(\n",
    "        by=[\"ReservoirRootName\", \"IsRelease\", \"MasterStationName\"],\n",
    "        ascending=[True, True, True]\n",
    "    ).reset_index(drop=True)\n",
    "    final_df[\"DataStreamID\"] = [\"DS\" + str(i+1).zfill(2) for i in range(len(final_df))]\n",
    "\n",
    "    # Final export\n",
    "    export = final_df[[\n",
    "        \"NewSiteName\",\n",
    "        \"SiteID (New)\",\n",
    "        \"DataStreamID\",\n",
    "        \"MasterStationName\",\n",
    "        \"MasterStationID\",\n",
    "        \"UNITS_DESC_ENTRY\",\n",
    "        \"CollectionStationName\",\n",
    "        \"COLLECTION_SYSTEM\"\n",
    "    ]].rename(columns={\n",
    "        \"MasterStationName\": \"DIVERT_STATION_NAME (old)\",\n",
    "        \"MasterStationID\": \"Station_ID (old)\",\n",
    "        \"COLLECTION_SYSTEM\": \"CollectionSystemName\"\n",
    "    })\n",
    "\n",
    "    export.to_csv(\"Reservior_Station_Datastream_Metadata_HydroServer_USU_20250507.csv\", index=False)\n",
    "    print(\"âœ… Saved: Reservior_Station_Datastream_Metadata_HydroServer_USU_20250507.csv\")\n",
    "\n",
    "# --- MAIN ---\n",
    "def main():\n",
    "    print(\"ðŸ”Ž Querying metadata...\")\n",
    "    metadata_df = query_station_metadata()\n",
    "    print(f\"ðŸ“¦ {len(metadata_df)} rows retrieved.\")\n",
    "\n",
    "    print(\"ðŸ§  Structuring export with release/non-release logic...\")\n",
    "    generate_final_export(metadata_df)\n",
    "\n",
    "    print(\"âœ… Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f66d671-9f6d-4bad-bbcb-f9340c15cfbf",
   "metadata": {},
   "source": [
    "### Following code takes the output datastreams csv file from above \"\"Reservior_Station_Datastream_Metadata_HydroServer_USU_20250507.csv\" and creates the site formated metadata directed by the USU team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737d14b-86e3-4fa9-b9fd-2499c9c5ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the structured metadata\n",
    "df = pd.read_csv(\"Reservior_Station_Datastream_Metadata_HydroServer_USU_20250507.csv\")\n",
    "\n",
    "# Load the original SQL metadata (used to retrieve LAT, LON, SYSTEM_NAME)\n",
    "original_metadata = query_station_metadata()  # or read from a saved CSV if you prefer\n",
    "\n",
    "# Match rows by Station_ID\n",
    "merged = df.merge(\n",
    "    original_metadata[[\"MasterStationID\", \"LAT\", \"LON\", \"SYSTEM_NAME\",\"COLLECTION_SYSTEM\"]],\n",
    "    left_on=\"Station_ID (old)\",\n",
    "    right_on=\"MasterStationID\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Determine SiteType\n",
    "merged[\"SiteType\"] = merged[\"DIVERT_STATION_NAME (old)\"].str.upper().apply(\n",
    "    lambda x: \"Reservoir Release\" if \"RELEASE\" in x else \"Reservoir\"\n",
    ")\n",
    "\n",
    "# Group by SiteID and take the first entry (since NewSiteName, SiteID, LAT/LON are consistent per site)\n",
    "site_meta = (\n",
    "    merged.groupby(\"SiteID (New)\")\n",
    "    .first()\n",
    "    .reset_index()\n",
    "    [[\n",
    "        \"NewSiteName\",\n",
    "        \"SiteID (New)\",\n",
    "        \"SiteType\",\n",
    "        \"LAT\",\n",
    "        \"LON\",\n",
    "        \"SYSTEM_NAME\",\n",
    "        \"COLLECTION_SYSTEM\"\n",
    "    ]]\n",
    ")\n",
    "\n",
    "# OPTIONAL: You can add County and River if you have a function or mapping\n",
    "# site_meta[\"County\"] = ...\n",
    "# site_meta[\"River\"] = ...\n",
    "\n",
    "# Rename SYSTEM_NAME and COLLECTION_SYSTEM for export\n",
    "site_meta = site_meta.rename(columns={\n",
    "    \"SYSTEM_NAME\": \"Workspace\",\n",
    "    \"COLLECTION_SYSTEM\": \"Collection_Systems\"\n",
    "})\n",
    "\n",
    "# Save output\n",
    "site_meta.to_csv(\"Reservior_Station_Sites_Metadata_HydroServer_USU_20250507.csv\", index=False)\n",
    "print(\"âœ… Saved: Reservior_Station_Sites_Metadata_HydroServer_USU_20250507.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eae833c-fcf9-4cdc-bbf2-da0d852607c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
