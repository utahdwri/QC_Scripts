{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "735e8cf9-fbe8-48c4-8a50-60d782a245ec",
   "metadata": {},
   "source": [
    "### Code to get all reservior data in the ouput formate so stations ie content,elevation,station id, etc. \n",
    "### are next to each other in columns the output csv file and creates the site formated metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d55d0b-0605-4a3d-adfe-d0b11f89f6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "\n",
    "# --- SQL SERVER CONNECTION SETUP ---\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = '*******'\n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    ")\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# --- STEP 1: SQL METADATA QUERY ---\n",
    "def query_station_metadata():\n",
    "    sql_query = \"\"\"\n",
    "    SELECT  \n",
    "        LTRIM(RTRIM([COLLECTION_SYSTEM])) AS COLLECTION_SYSTEM,\n",
    "        LTRIM(RTRIM([collection_sys_description])) AS collection_sys_description,\n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        LTRIM(RTRIM([STATION_MASTER].[STATION_NAME])) AS MasterStationName,\n",
    "        LTRIM(RTRIM([COLLECTION_STATIONS].[STATION_NAME])) AS CollectionStationName,\n",
    "        LTRIM(RTRIM([COMMENTS])) AS COMMENTS,\n",
    "        LTRIM(RTRIM([SiteType])) AS SiteType,\n",
    "        LTRIM(RTRIM([ANALOG_CHANNEL])) AS ANALOG_CHANNEL,\n",
    "        LTRIM(RTRIM([SYSTEM_NAME])) AS SYSTEM_NAME,\n",
    "        LTRIM(RTRIM([DatasetType])) AS DatasetType,\n",
    "        LTRIM(RTRIM([MEASURING_DEVICE])) AS MEASURING_DEVICE,\n",
    "        LTRIM(RTRIM([DEVICE_TYPE])) AS DEVICE_TYPE,\n",
    "        LTRIM(RTRIM([STATUS])) AS STATUS,\n",
    "        [LAT], [LON],\n",
    "        LTRIM(RTRIM([DataEntryMethod])) AS DataEntryMethod,\n",
    "        LTRIM(RTRIM([Telemetry])) AS Telemetry,\n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', [STATION_MASTER].[STATION_ID]) AS StationPage,\n",
    "        [UNITS_MASTER].[UNITS_ID],\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[RECORD_TYPE])) AS RECORD_TYPE,\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_BASE])) AS UNITS_DESC_BASE,\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_ENTRY])) AS UNITS_DESC_ENTRY,\n",
    "        [UNITS_MASTER].[UNITS_MULTIPLIER],\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_REALTIME])) AS UNITS_DESC_REALTIME,\n",
    "        COUNT([RECORD_YEAR]) AS NoOfYears, \n",
    "        MIN([RECORD_YEAR]) AS StartYr, \n",
    "        MAX([RECORD_YEAR]) AS EndYr\n",
    "    FROM [dvrtDB].[dbo].[STATION_MASTER]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] \n",
    "        ON [COLLECTION_SYSTEMS].[collection_sys_id] = [STATION_MASTER].[STATION_ID]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_STATIONS] \n",
    "        ON [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] \n",
    "        ON [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[DAILY_RECORDS] \n",
    "        ON [STATION_MASTER].[STATION_ID] = [DAILY_RECORDS].[STATION_ID]\n",
    "    WHERE \n",
    "        [STATUS] = 'A' AND\n",
    "        [DatasetType] = 'Observational' AND\n",
    "        [DataEntryMethod] != 'Manual' AND\n",
    "        [DataEntryMethod] IS NOT NULL AND\n",
    "        ([LAT] IS NOT NULL OR [LON] IS NOT NULL) AND\n",
    "        ([LON] > '-115' OR [LON] < '36') AND\n",
    "        [LAT] > 0 AND\n",
    "        (\n",
    "            [STATION_MASTER].[STATION_NAME] LIKE '%Reservoir%' OR \n",
    "            [COLLECTION_STATIONS].[STATION_NAME] LIKE '%Reservoir%'\n",
    "        )\n",
    "    GROUP BY\n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], [STATION_MASTER].[STATION_ID],\n",
    "        [STATION_MASTER].[STATION_NAME], [COLLECTION_STATIONS].[STATION_NAME],\n",
    "        [COMMENTS], [SiteType], [ANALOG_CHANNEL], [SYSTEM_NAME], [DatasetType],\n",
    "        [MEASURING_DEVICE], [DEVICE_TYPE], [STATUS], [LAT], [LON],\n",
    "        [DataEntryMethod], [Telemetry],\n",
    "        [UNITS_MASTER].[UNITS_ID], [UNITS_MASTER].[RECORD_TYPE],\n",
    "        [UNITS_MASTER].[UNITS_DESC_BASE], [UNITS_MASTER].[UNITS_DESC_ENTRY],\n",
    "        [UNITS_MASTER].[UNITS_MULTIPLIER], [UNITS_MASTER].[UNITS_DESC_REALTIME]\n",
    "    ORDER BY [STATION_MASTER].[STATION_ID]\n",
    "    \"\"\"\n",
    "    return pd.read_sql(sql_query, engine)\n",
    "\n",
    "# --- STEP 2: FINAL EXPORT LOGIC WITH TRANSPOSE + CHECK/COMMENT ---\n",
    "def generate_final_export(df):\n",
    "    df = df[df[\"SiteType\"].isin([\"Reservoir\", \"Reservoir Release\"])].copy()\n",
    "    df[\"IsRelease\"] = df[\"MasterStationName\"].str.upper().str.contains(\"RELEASE\")\n",
    "\n",
    "    def extract_base_name(name):\n",
    "        name = str(name).upper()\n",
    "        name = re.sub(r'\\b(USBR|USGS|RESERVOIR|RELEASE|CONTENTS|ELEVATION|STORAGE|POOL|EVAPORATION|LEVEL|GAGE HEIGHT)\\b', '', name)\n",
    "        name = re.sub(r'[^A-Z0-9 ]+', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name.split()[0].title() if name else \"Unknown\"\n",
    "\n",
    "    df[\"ReservoirRootName\"] = df[\"MasterStationName\"].apply(extract_base_name)\n",
    "\n",
    "    release_df = df[df[\"IsRelease\"]].copy()\n",
    "    nonrelease_df = df[~df[\"IsRelease\"]].copy()\n",
    "\n",
    "    unique_nonreleases = nonrelease_df[\"ReservoirRootName\"].unique()\n",
    "    nonrelease_group_map = {name: f\"PD{str(i+1).zfill(3)}\" for i, name in enumerate(unique_nonreleases)}\n",
    "    nonrelease_df[\"SiteID (New)\"] = nonrelease_df[\"ReservoirRootName\"].map(nonrelease_group_map)\n",
    "\n",
    "    release_df = release_df.reset_index(drop=True)\n",
    "    release_df[\"SiteID (New)\"] = [\"PD\" + str(i + len(nonrelease_group_map) + 1).zfill(3) for i in range(len(release_df))]\n",
    "\n",
    "    final_df = pd.concat([nonrelease_df, release_df], ignore_index=True)\n",
    "\n",
    "    def make_site_name(row):\n",
    "        base = row[\"ReservoirRootName\"]\n",
    "        system = row[\"SYSTEM_NAME\"].strip().title()\n",
    "        if row[\"IsRelease\"]:\n",
    "            return f\"{base} River Below {base} Reservoir, {system}, Near {base}\"\n",
    "        else:\n",
    "            return f\"{base} Reservoir, {system}, Near {base}\"\n",
    "\n",
    "    final_df[\"NewSiteName\"] = final_df.apply(make_site_name, axis=1)\n",
    "    final_df = final_df.sort_values(by=[\"ReservoirRootName\", \"IsRelease\", \"MasterStationName\"]).reset_index(drop=True)\n",
    "    final_df[\"DataStreamID\"] = [\"DS\" + str(i+1).zfill(2) for i in range(len(final_df))]\n",
    "\n",
    "    export = final_df[[\n",
    "        \"NewSiteName\", \"SiteID (New)\", \"DataStreamID\",\n",
    "        \"MasterStationName\", \"MasterStationID\", \"UNITS_DESC_ENTRY\",\n",
    "        \"CollectionStationName\", \"COLLECTION_SYSTEM\"\n",
    "    ]].rename(columns={\n",
    "        \"MasterStationName\": \"DIVERT_STATION_NAME (old)\",\n",
    "        \"MasterStationID\": \"Station_ID (old)\",\n",
    "        \"COLLECTION_SYSTEM\": \"CollectionSystemName\"\n",
    "    })\n",
    "\n",
    "    export[\"RecordNum\"] = export.groupby([\"NewSiteName\", \"SiteID (New)\"]).cumcount() + 1\n",
    "    id_cols = [\"NewSiteName\", \"SiteID (New)\"]\n",
    "    cols_to_expand = [\n",
    "        \"DIVERT_STATION_NAME (old)\", \"Station_ID (old)\",\n",
    "        \"UNITS_DESC_ENTRY\", \"CollectionStationName\", \"CollectionSystemName\"\n",
    "    ]\n",
    "\n",
    "    transposed = export[id_cols].drop_duplicates().copy()\n",
    "    for col in cols_to_expand:\n",
    "        pivoted = export.pivot_table(index=id_cols, columns=\"RecordNum\", values=col, aggfunc=\"first\")\n",
    "        pivoted.columns = [f\"{col}{i}\" for i in pivoted.columns]\n",
    "        transposed = transposed.merge(pivoted, on=id_cols, how=\"left\")\n",
    "\n",
    "    transposed[\"Check\"] = \"\"\n",
    "    transposed[\"Comment\"] = \"\"\n",
    "\n",
    "    transposed.to_csv(\"Reservior_Station_Datastream_Metadata_Transposed_20250513.csv\", index=False)\n",
    "    print(\"âœ… Saved: Reservior_Station_Datastream_Metadata_Transposed_20250513.csv\")\n",
    "\n",
    "# --- MAIN ---\n",
    "def main():\n",
    "    print(\"ðŸ”Ž Querying metadata...\")\n",
    "    metadata_df = query_station_metadata()\n",
    "    print(f\"ðŸ“¦ {len(metadata_df)} rows retrieved.\")\n",
    "    print(\"ðŸ§  Structuring export with release/non-release logic and transposing...\")\n",
    "    generate_final_export(metadata_df)\n",
    "    print(\"âœ… Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c268a2a-e906-45e6-8447-af627de4c69f",
   "metadata": {},
   "source": [
    "### Code to get all reservior data in the ouput formate so stations ie content,elevation, etc. \n",
    "### are next to each other in columns the output csv file and creates the site formated metadata \n",
    "### same code as aboved but this output code allows the xlsx file to have a drop down column \n",
    "### of approved and not approved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0843d8ad-5595-48c9-a2e0-f91903d40a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.worksheet.datavalidation import DataValidation\n",
    "\n",
    "# --- SQL SERVER CONNECTION SETUP ---\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = '*****'\n",
    "\n",
    "params = urllib.parse.quote_plus(\n",
    "    f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "    f\"SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n",
    ")\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "\n",
    "# --- STEP 1: SQL METADATA QUERY ---\n",
    "def query_station_metadata():\n",
    "    sql_query = \"\"\"\n",
    "    SELECT  \n",
    "        LTRIM(RTRIM([COLLECTION_SYSTEM])) AS COLLECTION_SYSTEM,\n",
    "        LTRIM(RTRIM([collection_sys_description])) AS collection_sys_description,\n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        LTRIM(RTRIM([STATION_MASTER].[STATION_NAME])) AS MasterStationName,\n",
    "        LTRIM(RTRIM([COLLECTION_STATIONS].[STATION_NAME])) AS CollectionStationName,\n",
    "        LTRIM(RTRIM([COMMENTS])) AS COMMENTS,\n",
    "        LTRIM(RTRIM([SiteType])) AS SiteType,\n",
    "        LTRIM(RTRIM([ANALOG_CHANNEL])) AS ANALOG_CHANNEL,\n",
    "        LTRIM(RTRIM([SYSTEM_NAME])) AS SYSTEM_NAME,\n",
    "        LTRIM(RTRIM([DatasetType])) AS DatasetType,\n",
    "        LTRIM(RTRIM([MEASURING_DEVICE])) AS MEASURING_DEVICE,\n",
    "        LTRIM(RTRIM([DEVICE_TYPE])) AS DEVICE_TYPE,\n",
    "        LTRIM(RTRIM([STATUS])) AS STATUS,\n",
    "        [LAT], [LON],\n",
    "        LTRIM(RTRIM([DataEntryMethod])) AS DataEntryMethod,\n",
    "        LTRIM(RTRIM([Telemetry])) AS Telemetry,\n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', [STATION_MASTER].[STATION_ID]) AS StationPage,\n",
    "        [UNITS_MASTER].[UNITS_ID],\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[RECORD_TYPE])) AS RECORD_TYPE,\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_BASE])) AS UNITS_DESC_BASE,\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_ENTRY])) AS UNITS_DESC_ENTRY,\n",
    "        [UNITS_MASTER].[UNITS_MULTIPLIER],\n",
    "        LTRIM(RTRIM([UNITS_MASTER].[UNITS_DESC_REALTIME])) AS UNITS_DESC_REALTIME,\n",
    "        COUNT([RECORD_YEAR]) AS NoOfYears, \n",
    "        MIN([RECORD_YEAR]) AS StartYr, \n",
    "        MAX([RECORD_YEAR]) AS EndYr\n",
    "    FROM [dvrtDB].[dbo].[STATION_MASTER]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] \n",
    "        ON [COLLECTION_SYSTEMS].[collection_sys_id] = [STATION_MASTER].[STATION_ID]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_STATIONS] \n",
    "        ON [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] \n",
    "        ON [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[DAILY_RECORDS] \n",
    "        ON [STATION_MASTER].[STATION_ID] = [DAILY_RECORDS].[STATION_ID]\n",
    "    WHERE \n",
    "        [STATUS] = 'A' AND\n",
    "        [DatasetType] = 'Observational' AND\n",
    "        [DataEntryMethod] != 'Manual' AND\n",
    "        [DataEntryMethod] IS NOT NULL AND\n",
    "        ([LAT] IS NOT NULL OR [LON] IS NOT NULL) AND\n",
    "        ([LON] > '-115' OR [LON] < '36') AND\n",
    "        [LAT] > 0 AND\n",
    "        (\n",
    "            [STATION_MASTER].[STATION_NAME] LIKE '%Reservoir%' OR \n",
    "            [COLLECTION_STATIONS].[STATION_NAME] LIKE '%Reservoir%'\n",
    "        )\n",
    "    GROUP BY\n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], [STATION_MASTER].[STATION_ID],\n",
    "        [STATION_MASTER].[STATION_NAME], [COLLECTION_STATIONS].[STATION_NAME],\n",
    "        [COMMENTS], [SiteType], [ANALOG_CHANNEL], [SYSTEM_NAME], [DatasetType],\n",
    "        [MEASURING_DEVICE], [DEVICE_TYPE], [STATUS], [LAT], [LON],\n",
    "        [DataEntryMethod], [Telemetry],\n",
    "        [UNITS_MASTER].[UNITS_ID], [UNITS_MASTER].[RECORD_TYPE],\n",
    "        [UNITS_MASTER].[UNITS_DESC_BASE], [UNITS_MASTER].[UNITS_DESC_ENTRY],\n",
    "        [UNITS_MASTER].[UNITS_MULTIPLIER], [UNITS_MASTER].[UNITS_DESC_REALTIME]\n",
    "    ORDER BY [STATION_MASTER].[STATION_ID]\n",
    "    \"\"\"\n",
    "    return pd.read_sql(sql_query, engine)\n",
    "\n",
    "# --- STEP 2: FINAL EXPORT LOGIC ---\n",
    "def generate_final_export(df):\n",
    "    df = df[df[\"SiteType\"].isin([\"Reservoir\", \"Reservoir Release\"])].copy()\n",
    "    df[\"IsRelease\"] = df[\"MasterStationName\"].str.upper().str.contains(\"RELEASE\")\n",
    "\n",
    "    def extract_base_name(name):\n",
    "        name = str(name).upper()\n",
    "        name = re.sub(r'\\b(USBR|USGS|RESERVOIR|RELEASE|CONTENTS|ELEVATION|STORAGE|POOL|EVAPORATION|LEVEL|GAGE HEIGHT)\\b', '', name)\n",
    "        name = re.sub(r'[^A-Z0-9 ]+', '', name)\n",
    "        name = re.sub(r'\\s+', ' ', name).strip()\n",
    "        return name.split()[0].title() if name else \"Unknown\"\n",
    "\n",
    "    df[\"ReservoirRootName\"] = df[\"MasterStationName\"].apply(extract_base_name)\n",
    "\n",
    "    release_df = df[df[\"IsRelease\"]].copy()\n",
    "    nonrelease_df = df[~df[\"IsRelease\"]].copy()\n",
    "\n",
    "    unique_nonreleases = nonrelease_df[\"ReservoirRootName\"].unique()\n",
    "    nonrelease_group_map = {name: f\"PD{str(i+1).zfill(3)}\" for i, name in enumerate(unique_nonreleases)}\n",
    "    nonrelease_df[\"SiteID (New)\"] = nonrelease_df[\"ReservoirRootName\"].map(nonrelease_group_map)\n",
    "\n",
    "    release_df = release_df.reset_index(drop=True)\n",
    "    release_df[\"SiteID (New)\"] = [\"PD\" + str(i + len(nonrelease_group_map) + 1).zfill(3) for i in range(len(release_df))]\n",
    "\n",
    "    final_df = pd.concat([nonrelease_df, release_df], ignore_index=True)\n",
    "\n",
    "    def make_site_name(row):\n",
    "        base = row[\"ReservoirRootName\"]\n",
    "        system = row[\"SYSTEM_NAME\"].strip().title()\n",
    "        if row[\"IsRelease\"]:\n",
    "            return f\"{base} River Below {base} Reservoir, {system}, Near {base}\"\n",
    "        else:\n",
    "            return f\"{base} Reservoir, {system}, Near {base}\"\n",
    "\n",
    "    final_df[\"NewSiteName\"] = final_df.apply(make_site_name, axis=1)\n",
    "    final_df = final_df.sort_values(by=[\"ReservoirRootName\", \"IsRelease\", \"MasterStationName\"]).reset_index(drop=True)\n",
    "    final_df[\"DataStreamID\"] = [\"DS\" + str(i+1).zfill(2) for i in range(len(final_df))]\n",
    "\n",
    "    export = final_df[[\n",
    "        \"NewSiteName\", \"SiteID (New)\", \"DataStreamID\",\n",
    "        \"MasterStationName\", \"MasterStationID\", \"UNITS_DESC_ENTRY\",\n",
    "        \"CollectionStationName\", \"COLLECTION_SYSTEM\"\n",
    "    ]].rename(columns={\n",
    "        \"MasterStationName\": \"DIVERT_STATION_NAME (old)\",\n",
    "        \"MasterStationID\": \"Station_ID (old)\",\n",
    "        \"COLLECTION_SYSTEM\": \"CollectionSystemName\"\n",
    "    })\n",
    "\n",
    "    export[\"RecordNum\"] = export.groupby([\"NewSiteName\", \"SiteID (New)\"]).cumcount() + 1\n",
    "    id_cols = [\"NewSiteName\", \"SiteID (New)\"]\n",
    "    cols_to_expand = [\n",
    "        \"DIVERT_STATION_NAME (old)\", \"Station_ID (old)\",\n",
    "        \"UNITS_DESC_ENTRY\", \"CollectionStationName\", \"CollectionSystemName\"\n",
    "    ]\n",
    "\n",
    "    transposed = export[id_cols].drop_duplicates().copy()\n",
    "    for col in cols_to_expand:\n",
    "        pivoted = export.pivot_table(index=id_cols, columns=\"RecordNum\", values=col, aggfunc=\"first\")\n",
    "        pivoted.columns = [f\"{col}{i}\" for i in pivoted.columns]\n",
    "        transposed = transposed.merge(pivoted, on=id_cols, how=\"left\")\n",
    "\n",
    "    transposed[\"Check\"] = \"\"\n",
    "    transposed[\"Comment\"] = \"\"\n",
    "\n",
    "    excel_path = \"Reservior_Station_Datastream_Metadata_Transposed_2.0_20250513.xlsx\"\n",
    "    transposed.to_excel(excel_path, index=False)\n",
    "    add_dropdown_to_excel(excel_path)\n",
    "\n",
    "# --- STEP 3: ADD DROPDOWN TO CHECK COLUMN ---\n",
    "def add_dropdown_to_excel(file_path, sheet_name=\"Sheet1\"):\n",
    "    wb = load_workbook(file_path)\n",
    "    ws = wb[sheet_name]\n",
    "    dv = DataValidation(type=\"list\", formula1='\"Approved,Not Approved\"', allow_blank=True)\n",
    "\n",
    "    for col in ws.iter_cols(1, ws.max_column):\n",
    "        if col[0].value == \"Check\":\n",
    "            col_letter = col[0].column_letter\n",
    "            dv.ranges.add(f\"{col_letter}2:{col_letter}{ws.max_row}\")\n",
    "            break\n",
    "\n",
    "    ws.add_data_validation(dv)\n",
    "    wb.save(file_path)\n",
    "    print(\"âœ… Dropdown added to 'Check' column.\")\n",
    "\n",
    "# --- MAIN ---\n",
    "def main():\n",
    "    print(\"ðŸ”Ž Querying metadata...\")\n",
    "    metadata_df = query_station_metadata()\n",
    "    print(f\"ðŸ“¦ {len(metadata_df)} rows retrieved.\")\n",
    "    print(\"ðŸ§  Structuring export with release/non-release logic and transposing...\")\n",
    "    generate_final_export(metadata_df)\n",
    "    print(\"âœ… Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92498977-7525-4581-97dd-4a2c5354bb38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
