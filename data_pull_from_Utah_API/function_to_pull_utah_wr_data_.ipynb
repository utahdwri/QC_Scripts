{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f16d326f-9f59-4fc9-ac76-3001cac02edd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2506649436.py, line 177)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 177\u001b[1;36m\u001b[0m\n\u001b[1;33m    comb_dir=os.path.join(\"..\", \"output\", \"wyseo_combined_data\")):\u001b[0m\n\u001b[1;37m                                                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \"\"\"\n",
    "    Function to pull diversion records from Utah Department of Water Resources website \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dst_dir : str\n",
    "        relative path location to directory to save downloaded data\n",
    "        (default is \"utdwr_raw_data\")\n",
    "\n",
    "    sites_ifp : str\n",
    "        relative path location to csv file containing all UCRB diversion sites.\n",
    "        This function only attempts to pull data for records with \"dataSource\" attribute of \"UTDWR\"\n",
    "        (default is \"ucrb_diversion_master_table.csv\")\n",
    "\n",
    "    sp_df : pandas dataframe\n",
    "        dataframe containing one record per day within period of interest, used for combining automatically\n",
    "        retrieved data with manually retrieved data located in hst_dir directory\n",
    "\n",
    "    hst_dir : str\n",
    "        relative path location to directory containing manually-retrieved historical records\n",
    "        (default is \"utdwr_historical_data\")\n",
    "\n",
    "    comb_dir : str\n",
    "        relative path location to directory where combined automatically-retrieved and manually-retrieved\n",
    "        records will be saved\n",
    "        (default is \"utdwr_combined_data\")\n",
    "\n",
    "    Exports\n",
    "    ----------\n",
    "    Microsoft Excel CSV file \"utdwr_diversion_sites.csv\" containing site information of every site\n",
    "    for which daily records were pulled\n",
    "\n",
    "    1 additional CSV file for each site (e.g. \"cms_ut_caineville_canal.csv\") containing daily diversion records of that site\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Need to add    \n",
    "    \"\"\"    \n",
    "    print(\"downloading UTDWR diversion record data to directory {0}\".format(dst_dir))\n",
    "    if os.path.exists(dst_dir):\n",
    "        pass\n",
    "        # shutil.rmtree(dst_dir)\n",
    "        # print(\"existing diversion data directory found and will be replaced\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir)\n",
    "\n",
    "    print(\"combining UTDWR diversion records into directory {0}\".format(comb_dir))\n",
    "    if os.path.exists(comb_dir):\n",
    "        pass\n",
    "        # shutil.rmtree(comb_dir)\n",
    "        # print(\"existing directory for combined data found and will be replaced\")\n",
    "    else:\n",
    "        os.mkdir(comb_dir)\n",
    "    \n",
    "    # organize info for lookup table\n",
    "    siteIds = []\n",
    "    siteNames = []\n",
    "    siteLat = []\n",
    "    siteLong = []\n",
    "    siteSource = []\n",
    "    siteFiles = []\n",
    "    siteUse = []\n",
    "    siteStart = []\n",
    "    siteEnd = []\n",
    "    noFillYears = []\n",
    "    shortID = []\n",
    "    destinationCode = []\n",
    "    destinationFlag = []\n",
    "\n",
    "    # import table of UTDWR diversion sites\n",
    "    sites = pd.read_csv(sites_ifp)\n",
    "    sites = sites.loc[sites[\"dataSource\"] == \"UTDWR\"].copy()\n",
    "    \n",
    "    # retrieve UT DWR data\n",
    "    for i, r in sites.loc[sites[\"utdwrID\"].notnull()].iterrows():\n",
    "        ID = r.utdwrID\n",
    "        siteFile = \"{0}.csv\".format(r.siteName)\n",
    "        \n",
    "        today = date.today()\n",
    "        Current_Date = today.strftime(\"%Y\")\n",
    "        URL = f\"https://www.waterrights.utah.gov/cgi-bin/dvrtview.exe?STATION_ID={ID}&RECORD_YEAR={Current_Date}&Modinfo=Daily_Comma\"\n",
    "        \n",
    "        try:\n",
    "            rr = requests.get(URL)\n",
    "            temp=StringIO(rr.text)\n",
    "            temp1=temp.readlines()\n",
    "            for line in temp1:\n",
    "                if line.startswith(\"Daily comma delimited\"):\n",
    "                    URL_raw= re.findall('\"([^\"]*)\"', line)\n",
    "            URL_end=\"\".join(map(str,URL_raw))\n",
    "            URL_base = 'https://www.waterrights.utah.gov'\n",
    "            URL_full = f\"{URL_base}{URL_end}\"\n",
    "            rrr = requests.get(URL_full)\n",
    "            temp_cd = StringIO(rrr.text)\n",
    "            df = pd.read_csv(temp_cd)\n",
    "            df.columns=[\"year\", \"month\", \"day\", \"discharge_cfs\"]\n",
    "            df.loc[:,\"date\"]=pd.to_datetime(df[['year','month', 'day']])\n",
    "            df.drop(labels=df.columns.difference([\"date\", \"discharge_cfs\"]), axis=1, inplace=True)\n",
    "            df.index=df.pop(\"date\")\n",
    "\n",
    "            df.to_csv(os.path.join(dst_dir, siteFile))\n",
    "\n",
    "            if r.historicalRecord == \"y\":\n",
    "                try:\n",
    "                    temp = pd.read_csv(os.path.join(hst_dir, \"{0}.csv\".format(r.siteName)))\n",
    "                except:\n",
    "                    temp = pd.read_csv(os.path.join(hst_dir1, \"{0}.csv\".format(r.siteName)))\n",
    "\n",
    "                temp.loc[:,\"date\"] = pd.to_datetime(temp.loc[:,\"date\"])\n",
    "                temp.index = temp.pop(\"date\")\n",
    "                temp = sp_df.join(temp,how=\"left\")\n",
    "\n",
    "                for ii, rr in temp.loc[temp[\"monthly_cfsd\"].notnull()].iterrows():\n",
    "                    ix = temp.loc[(temp[\"year\"] == rr.year) & (temp[\"month\"] == rr.month)].index\n",
    "                    temp.loc[ix, \"discharge_cfs\"] = rr.monthly_cfsd / monthrange(int(rr.year), int(rr.month))[1]\n",
    "\n",
    "                df.rename(columns={\"discharge_cfs\": \"auto_cfs\"}, inplace=True)\n",
    "\n",
    "                df = temp.join(df, how=\"left\")\n",
    "\n",
    "                df.loc[:, 'discharge_cfs'] = df.loc[:, 'discharge_cfs'].fillna(df.loc[:, 'auto_cfs'])\n",
    "                df.loc[:, \"date\"] = df.index.values\n",
    "                df.index = df.pop(\"date\")\n",
    "\n",
    "                df.filter(['discharge_cfs']).to_csv(os.path.join(comb_dir, siteFile))\n",
    "            else:\n",
    "                df.to_csv(os.path.join(comb_dir, siteFile))\n",
    "\n",
    "            print(r.siteName)\n",
    "            siteIds.append(r.utdwrID)\n",
    "            siteNames.append(r.siteName)\n",
    "            siteUse.append(r.siteUse)\n",
    "            siteLat.append(r.decLat)\n",
    "            siteLong.append(r.decLong)\n",
    "            siteSource.append(r.dataSource)\n",
    "            siteFiles.append(siteFile)\n",
    "            siteStart.append(r.startDate)\n",
    "            siteEnd.append(r.endDate)\n",
    "            noFillYears.append(r.no_fill_years)\n",
    "            shortID.append(r.shortID)\n",
    "            destinationCode.append(r.destinationCode)\n",
    "            destinationFlag.append(r.destinationFlag)\n",
    "\n",
    "        except:\n",
    "            print(\"could not download or process data from UTDWR diversion site: {0}\".format(r.siteName))\n",
    "            pass   \n",
    "            \n",
    "    # build and export diversion site lookup table for use in build_diversion_tabfiles()\n",
    "    df = pd.DataFrame(data={\"siteID\": siteIds, \"siteName\": siteNames, \"siteUse\": siteUse,\n",
    "                            \"siteLat\": siteLat, \"siteLong\": siteLong,\n",
    "                            \"siteSource\": siteSource, \"siteFile\": siteFiles,\n",
    "                            \"startDate\": siteStart, \"endDate\": siteEnd,\n",
    "                            \"noFillYears\": noFillYears, \"shortID\": shortID,\n",
    "                            \"destinationCode\": destinationCode, \"destinationFlag\": destinationFlag})\n",
    "    \n",
    "    df.loc[:, \"siteFolder\"] = os.path.split(comb_dir)[-1]\n",
    "    df_out = format_sites_df(df)\n",
    "    df_out.to_csv(os.path.join(dst_dir, \"..\", \"utdwr_diversion_sites.csv\"))\n",
    "\n",
    "\n",
    "    def get_wy_diversion_data(dst_dir=os.path.join(\"..\", \"output\", \"wyseo_raw_data\"),\n",
    "                          sp_df=None,\n",
    "                          sites_ifp=os.path.join(\"..\", \"input\", \"ucrb_diversion_master_table.csv\"),\n",
    "                          hst_dir=os.path.join(\"..\", \"input\", \"wyseo_historical_data\"),\n",
    "                          comb_dir=os.path.join(\"..\", \"output\", \"wyseo_combined_data\")):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214b80f-b426-462b-9993-8e185067a387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
