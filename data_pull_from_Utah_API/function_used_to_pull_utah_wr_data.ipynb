{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c6acb9-0791-4b93-a4e0-5f47879aa98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Usage:\n",
    "    To run the script, use the following command in the terminal:\n",
    "    python ucrb_diversions.py\n",
    "\n",
    "    Make sure to have the environment ucrb_diversions installed (see ucrb_diversions.yml in scripts folder)\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "# sys.path.append('../../ucrb_utils/python_packages_static')\n",
    "sys.path.append('python_packages_static')\n",
    "import os\n",
    "import shutil\n",
    "import platform\n",
    "import requests\n",
    "import re\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from calendar import monthrange\n",
    "import matplotlib.dates as mdates\n",
    "years1 = mdates.YearLocator()\n",
    "years5 = mdates.YearLocator(5)\n",
    "years10 = mdates.YearLocator(10)\n",
    "years20 = mdates.YearLocator(20)\n",
    "years_fmt = mdates.DateFormatter('%Y')\n",
    "from datetime import date\n",
    "\n",
    "#import spnspecs\n",
    "#spnspecs.set_graph_specifications()\n",
    "\n",
    "if 'window' in platform.platform().lower():\n",
    "    newln = '\\n'\n",
    "else:\n",
    "    newln = '\\r\\n'\n",
    "\n",
    "\n",
    "def divfilter(df=None, mindiff=25., minfact=2., sig=3.):\n",
    "    \"\"\"\n",
    "    Function to identify time series outliers using an initial and secondary filter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df : pandas dataframe\n",
    "        must contain two columns:\n",
    "            \"datetime\" of type np.datetime64\n",
    "            \"discharge_cfs\" of type float\n",
    "        (default is None)\n",
    "    \n",
    "    mindiff : float\n",
    "        minimum amount (in cfs) greater than the median annual max at which a record\n",
    "        can be deemed an outlier in the first pass filter\n",
    "        (default is 25.)\n",
    "\n",
    "    minfact : float\n",
    "        minimum factor by which a record must be greater than the median annual max\n",
    "        to be deemed an outlier in the first pass filter\n",
    "        (default is 2.)\n",
    "\n",
    "    sig : float\n",
    "        number of standard deviations above the median annual max at which a record can be deemed\n",
    "        an outlier in the second pass filter\n",
    "        (default is 3.)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        identical to input dataframe with additional columns:\n",
    "            \"mdflag\" of type int, where a value of 1 indicates records identified as outliers in the first pass filter\n",
    "            \"sdflag\" of type int, where a value of 1 indicates records identified as outliers in the second pass filter\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The first pass filter uses the median of the annual maximum values and a minimum difference and minimum factor\n",
    "    to identify potential outliers. Records meeting the criteria of potential outliers are not included in the\n",
    "    calculation of the standard deviation of the annual maximum values.\n",
    "\n",
    "    The second pass filter uses a provided sigma distance to identify potential outliers that are greater than a specified\n",
    "    number of standard deviations above the median annual maximum rate. The standard deviation value is  calculated on the\n",
    "    values from the first pass that did not meet the potential outlier criteria of minimum difference and minimum factor\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Need to add\n",
    "    \"\"\"\n",
    "\n",
    "    df.loc[:, \"year\"] = df.loc[:, \"datetime\"].apply(lambda x: x.year)\n",
    "    dfy = df.loc[df[\"discharge_cfs\"] > 0.].groupby(\"year\").max()\n",
    "    aym = np.median(dfy.loc[:, \"discharge_cfs\"])\n",
    "    \n",
    "    df.loc[:, \"mdflag\"] = 0\n",
    "    df.loc[df.apply(lambda x: all((x.discharge_cfs - aym > mindiff, x.discharge_cfs > minfact * aym)), axis=1), \"mdflag\"] = 1\n",
    "    \n",
    "    dfy = df.loc[(df[\"mdflag\"] == 0) & (df[\"discharge_cfs\"] > 0.)].groupby(\"year\").max()\n",
    "    aymstd = np.std(dfy.loc[:, \"discharge_cfs\"])\n",
    "    \n",
    "    df.loc[:, \"sdflag\"] = 0\n",
    "    df.loc[df.apply(lambda x: x.discharge_cfs > (aym + sig * aymstd), axis=1), \"sdflag\"] = 1\n",
    "    \n",
    "    return df, aym\n",
    "\n",
    "\n",
    "def gaplengths(vals):\n",
    "    \"\"\"\n",
    "    Function to identify the length of consecutive null values surrounding each null value in a series of values.\n",
    "    This function is used by fill_missing_diversion_values() to identify gaps to be filled that are less than 90 days.\n",
    "    Gaps longer than 90 days are not filled to avoid interpolating values from irrigation season into non-irrigation season.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    vals : list of float values\n",
    "        (default is None)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    cts : list of int values\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Need to add\n",
    "    \"\"\"\n",
    "    i0 = 0\n",
    "    x = 0\n",
    "    cts = []\n",
    "\n",
    "    for i,v in enumerate(vals):\n",
    "        if np.isnan(v):\n",
    "            x += 1\n",
    "            cts.append(x)\n",
    "        else:\n",
    "            cts[i0: i] = [x] * (i - i0)\n",
    "            i0 = i\n",
    "            x = 0\n",
    "            cts.append(x)\n",
    "    return cts\n",
    "\n",
    "\n",
    "def build_spdf(start=\"19791231\", end=\"20220930\", spfreq=\"M\", tsfreq=\"D\"):\n",
    "    \"\"\"\n",
    "    Function to build a dataframe of stress periods and time steps for a given date range and intervals\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    start : str\n",
    "        starting datetime in string format \"%Y%m%d\"\n",
    "        (default is \"19791231\")\n",
    "\n",
    "    end : str\n",
    "        ending datetime in string format \"%Y%m%d\"\n",
    "        (default is \"20220930\")\n",
    "\n",
    "    spfreq : str\n",
    "        stress period frequency. Must be \"M\"\n",
    "        (default is \"M\")\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        includes dates, stress periods assigned by month, and MODFLOW totim values for each day, in 1-index\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Need to add    \n",
    "    \"\"\"\n",
    "    dts = pd.date_range(start=pd.to_datetime(start), end=pd.to_datetime(end), freq=tsfreq)\n",
    "    df = pd.DataFrame(data={\"year\": dts.year, \"month\": dts.month, \"day\": dts.day}, index=dts)\n",
    "    if spfreq == \"M\":\n",
    "        df.loc[:, \"sp\"] = df.groupby([\"year\", \"month\"]).ngroup()\n",
    "        gr = df.groupby('sp')\n",
    "        df.loc[:, \"ts\"] = gr.cumcount()\n",
    "    df.loc[:, \"totim\"] = range(1, len(df) + 1)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def format_sites_df(df=None):\n",
    "    \"\"\"\n",
    "    Function to create point geometries from decimal lat/long and reproject to UTM 12N \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    df : pandas dataframe\n",
    "        function applies to diversion site dataframes created in each data pulling function\n",
    "        (default is None)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "        original dataframe modified to include point geometries for each site in UTM 12N projection\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Need to add    \n",
    "    \"\"\"\n",
    "    df.loc[:, \"geometry\"] = df.apply(lambda x: Point(x.siteLong, x.siteLat), axis=1)\n",
    "    df = gp.GeoDataFrame(df, geometry=\"geometry\", crs=\"epsg:4269\")\n",
    "    df.to_crs(epsg=model_epsg, inplace=True)\n",
    "    df.loc[:, \"utmX\"] = df.loc[:, \"geometry\"].apply(lambda xx: xx.x)\n",
    "    df.loc[:, \"utmY\"] = df.loc[:, \"geometry\"].apply(lambda xx: xx.y)\n",
    "    # df.drop([\"siteLat\", \"siteLong\"], axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cdss_diversion_data(dst_dir=os.path.join(\"..\", \"output\", \"cdss_raw_data\"),\n",
    "                            sites_ifp=os.path.join(\"..\", \"input\", \"ucrb_diversion_master_table.csv\"),\n",
    "                            apiKey=None):\n",
    "    \"\"\"\n",
    "    Function to pull diversion records from Utah Department of Water Resources website \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dst_dir : str\n",
    "        relative path location to directory to save downloaded data\n",
    "        (default is \"utdwr_raw_data\")\n",
    "\n",
    "    sites_ifp : str\n",
    "        relative path location to csv file containing all UCRB diversion sites.\n",
    "        This function only attempts to pull data for records with \"dataSource\" attribute of \"UTDWR\"\n",
    "        (default is \"ucrb_diversion_master_table.csv\")\n",
    "\n",
    "    sp_df : pandas dataframe\n",
    "        dataframe containing one record per day within period of interest, used for combining automatically\n",
    "        retrieved data with manually retrieved data located in hst_dir directory\n",
    "\n",
    "    hst_dir : str\n",
    "        relative path location to directory containing manually-retrieved historical records\n",
    "        (default is \"utdwr_historical_data\")\n",
    "\n",
    "    comb_dir : str\n",
    "        relative path location to directory where combined automatically-retrieved and manually-retrieved\n",
    "        records will be saved\n",
    "        (default is \"utdwr_combined_data\")\n",
    "\n",
    "    Exports\n",
    "    ----------\n",
    "    Microsoft Excel CSV file \"utdwr_diversion_sites.csv\" containing site information of every site\n",
    "    for which daily records were pulled\n",
    "\n",
    "    1 additional CSV file for each site (e.g. \"cms_ut_caineville_canal.csv\") containing daily diversion records of that site\n",
    "\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Need to add    \n",
    "    \"\"\"    \n",
    "    print(\"downloading UTDWR diversion record data to directory {0}\".format(dst_dir))\n",
    "    if os.path.exists(dst_dir):\n",
    "        pass\n",
    "        # shutil.rmtree(dst_dir)\n",
    "        # print(\"existing diversion data directory found and will be replaced\")\n",
    "    else:\n",
    "        os.mkdir(dst_dir)\n",
    "\n",
    "    print(\"combining UTDWR diversion records into directory {0}\".format(comb_dir))\n",
    "    if os.path.exists(comb_dir):\n",
    "        pass\n",
    "        # shutil.rmtree(comb_dir)\n",
    "        # print(\"existing directory for combined data found and will be replaced\")\n",
    "    else:\n",
    "        os.mkdir(comb_dir)\n",
    "    \n",
    "    # organize info for lookup table\n",
    "    siteIds = []\n",
    "    siteNames = []\n",
    "    siteLat = []\n",
    "    siteLong = []\n",
    "    siteSource = []\n",
    "    siteFiles = []\n",
    "    siteUse = []\n",
    "    siteStart = []\n",
    "    siteEnd = []\n",
    "    noFillYears = []\n",
    "    shortID = []\n",
    "    destinationCode = []\n",
    "    destinationFlag = []\n",
    "\n",
    "    # import table of UTDWR diversion sites\n",
    "    sites = pd.read_csv(sites_ifp)\n",
    "    sites = sites.loc[sites[\"dataSource\"] == \"UTDWR\"].copy()\n",
    "    \n",
    "    # retrieve UT DWR data\n",
    "    for i, r in sites.loc[sites[\"utdwrID\"].notnull()].iterrows():\n",
    "        ID = r.utdwrID\n",
    "        siteFile = \"{0}.csv\".format(r.siteName)\n",
    "        \n",
    "        today = date.today()\n",
    "        Current_Date = today.strftime(\"%Y\")\n",
    "        URL = f\"https://www.waterrights.utah.gov/cgi-bin/dvrtview.exe?STATION_ID={ID}&RECORD_YEAR={Current_Date}&Modinfo=Daily_Comma\"\n",
    "        \n",
    "        try:\n",
    "            rr = requests.get(URL)\n",
    "            temp=StringIO(rr.text)\n",
    "            temp1=temp.readlines()\n",
    "            for line in temp1:\n",
    "                if line.startswith(\"Daily comma delimited\"):\n",
    "                    URL_raw= re.findall('\"([^\"]*)\"', line)\n",
    "            URL_end=\"\".join(map(str,URL_raw))\n",
    "            URL_base = 'https://www.waterrights.utah.gov'\n",
    "            URL_full = f\"{URL_base}{URL_end}\"\n",
    "            rrr = requests.get(URL_full)\n",
    "            temp_cd = StringIO(rrr.text)\n",
    "            df = pd.read_csv(temp_cd)\n",
    "            df.columns=[\"year\", \"month\", \"day\", \"discharge_cfs\"]\n",
    "            df.loc[:,\"date\"]=pd.to_datetime(df[['year','month', 'day']])\n",
    "            df.drop(labels=df.columns.difference([\"date\", \"discharge_cfs\"]), axis=1, inplace=True)\n",
    "            df.index=df.pop(\"date\")\n",
    "\n",
    "            df.to_csv(os.path.join(dst_dir, siteFile))\n",
    "\n",
    "            if r.historicalRecord == \"y\":\n",
    "                try:\n",
    "                    temp = pd.read_csv(os.path.join(hst_dir, \"{0}.csv\".format(r.siteName)))\n",
    "                except:\n",
    "                    temp = pd.read_csv(os.path.join(hst_dir1, \"{0}.csv\".format(r.siteName)))\n",
    "\n",
    "                temp.loc[:,\"date\"] = pd.to_datetime(temp.loc[:,\"date\"])\n",
    "                temp.index = temp.pop(\"date\")\n",
    "                temp = sp_df.join(temp,how=\"left\")\n",
    "\n",
    "                for ii, rr in temp.loc[temp[\"monthly_cfsd\"].notnull()].iterrows():\n",
    "                    ix = temp.loc[(temp[\"year\"] == rr.year) & (temp[\"month\"] == rr.month)].index\n",
    "                    temp.loc[ix, \"discharge_cfs\"] = rr.monthly_cfsd / monthrange(int(rr.year), int(rr.month))[1]\n",
    "\n",
    "                df.rename(columns={\"discharge_cfs\": \"auto_cfs\"}, inplace=True)\n",
    "\n",
    "                df = temp.join(df, how=\"left\")\n",
    "\n",
    "                df.loc[:, 'discharge_cfs'] = df.loc[:, 'discharge_cfs'].fillna(df.loc[:, 'auto_cfs'])\n",
    "                df.loc[:, \"date\"] = df.index.values\n",
    "                df.index = df.pop(\"date\")\n",
    "\n",
    "                df.filter(['discharge_cfs']).to_csv(os.path.join(comb_dir, siteFile))\n",
    "            else:\n",
    "                df.to_csv(os.path.join(comb_dir, siteFile))\n",
    "\n",
    "            print(r.siteName)\n",
    "            siteIds.append(r.utdwrID)\n",
    "            siteNames.append(r.siteName)\n",
    "            siteUse.append(r.siteUse)\n",
    "            siteLat.append(r.decLat)\n",
    "            siteLong.append(r.decLong)\n",
    "            siteSource.append(r.dataSource)\n",
    "            siteFiles.append(siteFile)\n",
    "            siteStart.append(r.startDate)\n",
    "            siteEnd.append(r.endDate)\n",
    "            noFillYears.append(r.no_fill_years)\n",
    "            shortID.append(r.shortID)\n",
    "            destinationCode.append(r.destinationCode)\n",
    "            destinationFlag.append(r.destinationFlag)\n",
    "\n",
    "        except:\n",
    "            print(\"could not download or process data from UTDWR diversion site: {0}\".format(r.siteName))\n",
    "            pass   \n",
    "            \n",
    "    # build and export diversion site lookup table for use in build_diversion_tabfiles()\n",
    "    df = pd.DataFrame(data={\"siteID\": siteIds, \"siteName\": siteNames, \"siteUse\": siteUse,\n",
    "                            \"siteLat\": siteLat, \"siteLong\": siteLong,\n",
    "                            \"siteSource\": siteSource, \"siteFile\": siteFiles,\n",
    "                            \"startDate\": siteStart, \"endDate\": siteEnd,\n",
    "                            \"noFillYears\": noFillYears, \"shortID\": shortID,\n",
    "                            \"destinationCode\": destinationCode, \"destinationFlag\": destinationFlag})\n",
    "    \n",
    "    df.loc[:, \"siteFolder\"] = os.path.split(comb_dir)[-1]\n",
    "    df_out = format_sites_df(df)\n",
    "    df_out.to_csv(os.path.join(dst_dir, \"..\", \"utdwr_diversion_sites.csv\"))\n",
    "\n",
    "\n",
    "def get_wy_diversion_data(dst_dir=os.path.join(\"..\", \"output\", \"wyseo_raw_data\"),\n",
    "                          sp_df=None,\n",
    "                          sites_ifp=os.path.join(\"..\", \"input\", \"ucrb_diversion_master_table.csv\"),\n",
    "                          hst_dir=os.path.join(\"..\", \"input\", \"wyseo_historical_data\"),\n",
    "                          comb_dir=os.path.join(\"..\", \"output\", \"wyseo_combined_data\")\n",
    "                         ):\n",
    "    print(\"Function is defined correctly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37779be-28a9-4123-8a5e-e8dfcfe4a846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
