{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2800675b-f5e9-4321-a9a4-661d7190df50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to usgs_utah_sites.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    \"\"\"\n",
    "    Fetches USGS gage stations and reservoir levels in a given state.\n",
    "    :param state_code: The US state abbreviation (default: 'UT' for Utah)\n",
    "    :return: A list of site names and site numbers\n",
    "    \"\"\"\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'stateCd': state_code,\n",
    "        'siteStatus': 'all',\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    sites = []\n",
    "    \n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        sites.append((site_code, site_name))\n",
    "    \n",
    "    return sites\n",
    "\n",
    "# Fetch USGS gage stations and reservoir levels in Utah\n",
    "usgs_sites = get_usgs_sites()\n",
    "\n",
    "# Convert to DataFrame and export to CSV\n",
    "df = pd.DataFrame(usgs_sites, columns=['Site Code', 'Site Name'])\n",
    "df.to_csv('usgs_utah_sites.csv', index=False)\n",
    "\n",
    "print(\"Data exported to usgs_utah_sites.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "523bdf0d-c4c7-4b93-ab44-33a718e38802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to usgs_utah_sites.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    \"\"\"\n",
    "    Fetches unique USGS gage stations and reservoir levels in a given state.\n",
    "    :param state_code: The US state abbreviation (default: 'UT' for Utah)\n",
    "    :return: A list of unique site names and site numbers\n",
    "    \"\"\"\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'stateCd': state_code,\n",
    "        'siteStatus': 'all',\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    sites = set()  # Using a set to store unique (site_code, site_name) tuples\n",
    "    \n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        sites.add((site_code, site_name))  # Sets automatically remove duplicates\n",
    "    \n",
    "    return list(sites)  # Convert back to a list for DataFrame\n",
    "\n",
    "# Fetch unique USGS gage stations and reservoir levels in Utah\n",
    "usgs_sites = get_usgs_sites()\n",
    "\n",
    "# Convert to DataFrame and export to CSV\n",
    "df = pd.DataFrame(usgs_sites, columns=['Site Code', 'Site Name'])\n",
    "df.to_csv('usgs_utah_sites_removed_copy_Id.csv', index=False)\n",
    "\n",
    "print(\"Data exported to usgs_utah_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ad085f-3099-4b0c-bc37-382a8ef7bc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to usgs_utah_sites.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    \"\"\"\n",
    "    Fetches unique USGS gage stations and reservoir levels in a given state, \n",
    "    filtering out site codes with more than 8 digits.\n",
    "    \n",
    "    :param state_code: The US state abbreviation (default: 'UT' for Utah)\n",
    "    :return: A list of unique site names and site numbers with valid site codes\n",
    "    \"\"\"\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'stateCd': state_code,\n",
    "        'siteStatus': 'all',\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    sites = set()  # Using a set to store unique (site_code, site_name) tuples\n",
    "    \n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        \n",
    "        # Ensure site_code is numeric and does not exceed 8 digits\n",
    "        if site_code.isdigit() and len(site_code) <= 8:\n",
    "            sites.add((site_code, site_name))  # Sets automatically remove duplicates\n",
    "    \n",
    "    return list(sites)  # Convert back to a list for DataFrame\n",
    "\n",
    "# Fetch unique USGS gage stations and reservoir levels in Utah\n",
    "usgs_sites = get_usgs_sites()\n",
    "\n",
    "# Convert to DataFrame and export to CSV\n",
    "df = pd.DataFrame(usgs_sites, columns=['Site Code', 'Site Name'])\n",
    "df.to_csv('usgs_utah_sites_removed_8_digits.csv', index=False)\n",
    "\n",
    "print(\"Data exported to usgs_utah_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a7f15c-d21e-4f46-8e3a-149b655f1e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbenko\\AppData\\Local\\Temp\\1\\ipykernel_18472\\15565979.py:80: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_sql = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '\n    SELECT  \n        [COLLECTION_SYSTEM], [collection_sys_description], \n        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n        [SeriesVerifiedDate], \n        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n        STATION_MASTER.STATION_ID) AS StationPage, \n        COUNT([RECORD_YEAR]) AS NoOfYears, MIN([RECORD_YEAR]) AS StartYr, \n        MAX([RECORD_YEAR]) AS EndYr \n    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n    GROUP BY \n        [COLLECTION_SYSTEM], [collection_sys_description], \n        [STATION_MASTER].[STATION_ID], [STATION_MASTER].[STATION_NAME], \n        [COLLECTION_STATIONS].[STATION_NAME], [RETRIES], [SEQ_NO], \n        [COMMENTS], [SiteType], [COMMON_DESC], [DIVERTING_WORKS], \n        [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], [STATION_ID], [SYSTEM_NAME], \n        DatasetType, [MEASURING_DEVICE], [DEVICE_TYPE], [OWNER_PHONE], \n        [REALTIME_INCLUDE], [LAT], [STATUS], [LON], [LOW_FLOW], \n        [HIGH_FLOW], [SiteState], [UNITS_DESC_BASE], CAPTURE_SEQ_NO, \n        DataEntryMethod, [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n        [SeriesVerifiedDate], STATION_MASTER.STATION_ID\n    ': ('42S22', \"[42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207) (SQLExecDirectW); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42S22', \"[42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207) (SQLExecDirectW); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 86\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Fetch data\u001b[39;00m\n\u001b[0;32m     85\u001b[0m df_usgs \u001b[38;5;241m=\u001b[39m get_usgs_sites()\n\u001b[1;32m---> 86\u001b[0m df_sql \u001b[38;5;241m=\u001b[39m get_sql_data()\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Merge data by matching \"Site Code\" with \"CollectionStationName\" from SQL data\u001b[39;00m\n\u001b[0;32m     89\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m df_usgs\u001b[38;5;241m.\u001b[39mmerge(df_sql, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSite Code\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectionStationName\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 80\u001b[0m, in \u001b[0;36mget_sql_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Establish SQL connection\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyodbc\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDRIVER=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124mODBC Driver 17 for SQL Server\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSERVER=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPWD=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m---> 80\u001b[0m     df_sql \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query, conn)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_sql\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    707\u001b[0m             sql,\n\u001b[0;32m    708\u001b[0m             index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    709\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    710\u001b[0m             coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    711\u001b[0m             parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    712\u001b[0m             chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    713\u001b[0m             dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    714\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    715\u001b[0m         )\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '\n    SELECT  \n        [COLLECTION_SYSTEM], [collection_sys_description], \n        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n        [SeriesVerifiedDate], \n        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n        STATION_MASTER.STATION_ID) AS StationPage, \n        COUNT([RECORD_YEAR]) AS NoOfYears, MIN([RECORD_YEAR]) AS StartYr, \n        MAX([RECORD_YEAR]) AS EndYr \n    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n    GROUP BY \n        [COLLECTION_SYSTEM], [collection_sys_description], \n        [STATION_MASTER].[STATION_ID], [STATION_MASTER].[STATION_NAME], \n        [COLLECTION_STATIONS].[STATION_NAME], [RETRIES], [SEQ_NO], \n        [COMMENTS], [SiteType], [COMMON_DESC], [DIVERTING_WORKS], \n        [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], [STATION_ID], [SYSTEM_NAME], \n        DatasetType, [MEASURING_DEVICE], [DEVICE_TYPE], [OWNER_PHONE], \n        [REALTIME_INCLUDE], [LAT], [STATUS], [LON], [LOW_FLOW], \n        [HIGH_FLOW], [SiteState], [UNITS_DESC_BASE], CAPTURE_SEQ_NO, \n        DataEntryMethod, [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n        [SeriesVerifiedDate], STATION_MASTER.STATION_ID\n    ': ('42S22', \"[42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207) (SQLExecDirectW); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'RECORD_YEAR'. (207)\")"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Database connection parameters\n",
    "server = 'wrt-sql-prod'\n",
    "database = 'dvrtDB'\n",
    "username = 'wrtsqlq'\n",
    "password = 'guest'\n",
    "\n",
    "# Function to fetch USGS sites\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    sites = set()  # Store unique (site_code, site_name) pairs\n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "\n",
    "        # Filter: Only numeric site codes with max 8 digits\n",
    "        if site_code.isdigit() and len(site_code) <= 8:\n",
    "            sites.add((site_code, site_name))\n",
    "\n",
    "    return pd.DataFrame(list(sites), columns=['Site Code', 'Site Name'])\n",
    "\n",
    "# Function to fetch SQL data\n",
    "def get_sql_data():\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], \n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n",
    "        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n",
    "        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n",
    "        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n",
    "        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n",
    "        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n",
    "        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n",
    "        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n",
    "        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n",
    "        [SeriesVerifiedDate], \n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n",
    "        STATION_MASTER.STATION_ID) AS StationPage, \n",
    "        COUNT([RECORD_YEAR]) AS NoOfYears, MIN([RECORD_YEAR]) AS StartYr, \n",
    "        MAX([RECORD_YEAR]) AS EndYr \n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n",
    "        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n",
    "        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n",
    "        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    GROUP BY \n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], \n",
    "        [STATION_MASTER].[STATION_ID], [STATION_MASTER].[STATION_NAME], \n",
    "        [COLLECTION_STATIONS].[STATION_NAME], [RETRIES], [SEQ_NO], \n",
    "        [COMMENTS], [SiteType], [COMMON_DESC], [DIVERTING_WORKS], \n",
    "        [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], [STATION_ID], [SYSTEM_NAME], \n",
    "        DatasetType, [MEASURING_DEVICE], [DEVICE_TYPE], [OWNER_PHONE], \n",
    "        [REALTIME_INCLUDE], [LAT], [STATUS], [LON], [LOW_FLOW], \n",
    "        [HIGH_FLOW], [SiteState], [UNITS_DESC_BASE], CAPTURE_SEQ_NO, \n",
    "        DataEntryMethod, [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n",
    "        [SeriesVerifiedDate], STATION_MASTER.STATION_ID\n",
    "    \"\"\"\n",
    "\n",
    "    # Establish SQL connection\n",
    "    with pyodbc.connect(\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={server};\"\n",
    "        f\"DATABASE={database};\"\n",
    "        f\"UID={username};\"\n",
    "        f\"PWD={password}\"\n",
    "    ) as conn:\n",
    "        df_sql = pd.read_sql(query, conn)\n",
    "\n",
    "    return df_sql\n",
    "\n",
    "# Fetch data\n",
    "df_usgs = get_usgs_sites()\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Merge data by matching \"Site Code\" with \"CollectionStationName\" from SQL data\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites_sql_query.csv', index=False)\n",
    "\n",
    "print(\"Data successfully exported to usgs_utah_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab65f201-915e-4a3a-838b-6f7248df9cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbenko\\AppData\\Local\\Temp\\1\\ipykernel_18472\\4039272145.py:66: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_sql = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to usgs_utah_sites_with_sql_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "# Fetch USGS Sites\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    sites = []\n",
    "    \n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        \n",
    "        # Only include site codes with 8 or fewer digits\n",
    "        if site_code.isdigit() and len(site_code) <= 8:\n",
    "            sites.append((site_code, site_name))\n",
    "\n",
    "    return pd.DataFrame(sites, columns=['Site Code', 'Site Name']).drop_duplicates(subset=['Site Code'])\n",
    "\n",
    "# Fetch SQL Data\n",
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], \n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n",
    "        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n",
    "        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n",
    "        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n",
    "        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n",
    "        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n",
    "        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n",
    "        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n",
    "        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n",
    "        [SeriesVerifiedDate], \n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n",
    "        STATION_MASTER.STATION_ID) AS StationPage\n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n",
    "        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n",
    "        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n",
    "        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    \"\"\"\n",
    "\n",
    "    with pyodbc.connect(\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={server};\"\n",
    "        f\"DATABASE={database};\"\n",
    "        f\"UID={username};\"\n",
    "        f\"PWD={password}\"\n",
    "    ) as conn:\n",
    "        df_sql = pd.read_sql(query, conn)\n",
    "\n",
    "    return df_sql\n",
    "\n",
    "# Fetch data\n",
    "df_usgs = get_usgs_sites()\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Merge data by matching \"Site Code\" with \"CollectionStationName\" from SQL data\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites_with_sql_data.csv', index=False)\n",
    "print(\"Data exported to usgs_utah_sites_with_sql_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d11df4-b050-4b98-8fe4-91d334514a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL data loaded: 1129 rows\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'usgs_utah_sites_with_sql_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 82\u001b[0m\n\u001b[0;32m     79\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m df_usgs\u001b[38;5;241m.\u001b[39mmerge(df_sql, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSite Code\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectionStationName\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Export to CSV\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m df_merged\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124musgs_utah_sites_with_sql_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData exported to usgs_utah_sites_with_sql_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'usgs_utah_sites_with_sql_data.csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Fetch USGS Sites\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    sites = []\n",
    "    \n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        \n",
    "        # Only include site codes with 8 or fewer digits\n",
    "        if site_code.isdigit() and len(site_code) <= 8:\n",
    "            sites.append((site_code, site_name))\n",
    "\n",
    "    return pd.DataFrame(sites, columns=['Site Code', 'Site Name']).drop_duplicates(subset=['Site Code'])\n",
    "\n",
    "# Fetch SQL Data using SQLAlchemy\n",
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "    \n",
    "    # Create SQLAlchemy engine\n",
    "    conn_str = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    engine = create_engine(conn_str)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], \n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n",
    "        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n",
    "        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n",
    "        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n",
    "        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n",
    "        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n",
    "        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n",
    "        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n",
    "        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n",
    "        [SeriesVerifiedDate], \n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n",
    "        STATION_MASTER.STATION_ID) AS StationPage\n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n",
    "        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n",
    "        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n",
    "        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    \"\"\"\n",
    "\n",
    "    # Read SQL data\n",
    "    df_sql = pd.read_sql(query, engine)\n",
    "\n",
    "    return df_sql\n",
    "\n",
    "# Fetch data\n",
    "df_usgs = get_usgs_sites()\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Check if SQL data is empty before merging\n",
    "if df_sql.empty:\n",
    "    print(\"SQL query returned no data.\")\n",
    "else:\n",
    "    print(f\"SQL data loaded: {df_sql.shape[0]} rows\")\n",
    "\n",
    "# Merge data by matching \"Site Code\" with \"CollectionStationName\" from SQL data\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites_with_sql_data.csv', index=False)\n",
    "print(\"Data exported to usgs_utah_sites_with_sql_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56caabfe-9afd-4d8e-8470-a53c8031f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL data loaded: 1131 rows\n",
      "Data exported to usgs_utah_sites_with_sql_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Fetch USGS Sites\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    sites = []\n",
    "    \n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        \n",
    "        # Only include site codes with 8 or fewer digits\n",
    "        if site_code.isdigit() and len(site_code) <= 8:\n",
    "            sites.append((site_code, site_name))\n",
    "\n",
    "    return pd.DataFrame(sites, columns=['Site Code', 'Site Name']).drop_duplicates(subset=['Site Code'])\n",
    "\n",
    "# Fetch SQL Data using SQLAlchemy\n",
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "    \n",
    "    # Create SQLAlchemy engine\n",
    "    conn_str = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    engine = create_engine(conn_str)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], \n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n",
    "        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n",
    "        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n",
    "        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n",
    "        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n",
    "        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n",
    "        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n",
    "        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n",
    "        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n",
    "        [SeriesVerifiedDate], \n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n",
    "        STATION_MASTER.STATION_ID) AS StationPage\n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n",
    "        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n",
    "        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n",
    "        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    \"\"\"\n",
    "\n",
    "    # Read SQL data\n",
    "    df_sql = pd.read_sql(query, engine)\n",
    "\n",
    "    return df_sql\n",
    "\n",
    "# Fetch data\n",
    "df_usgs = get_usgs_sites()\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Check if SQL data is empty before merging\n",
    "if df_sql.empty:\n",
    "    print(\"SQL query returned no data.\")\n",
    "else:\n",
    "    print(f\"SQL data loaded: {df_sql.shape[0]} rows\")\n",
    "\n",
    "# Merge data by matching \"Site Code\" with \"CollectionStationName\" from SQL data\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites_with_sql_data.csv', index=False)\n",
    "print(\"Data exported to usgs_utah_sites_with_sql_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d21d195e-f146-4867-a53e-d71d328b6123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unmatched USGS Site Codes:\n",
      "     Site Code                                       Site Name\n",
      "0     09163675       COTTONWOOD WASH AT I-70, NEAR CISCO, UTAH\n",
      "1     09180000                    DOLORES RIVER NEAR CISCO, UT\n",
      "13    09180500                   COLORADO RIVER NEAR CISCO, UT\n",
      "33    09180920    ONION CREEK ABOVE ONION C BRIDGE NR MOAB, UT\n",
      "34    09180970  ONION CREEK BELOW ONION CRK BRIDGE NR MOAB, UT\n",
      "...        ...                                             ...\n",
      "1257  10241800             ASHDOWN CREEK NEAR CEDAR CITY, UTAH\n",
      "1259  10242300                PINTO CREEK NEAR NEWCASTLE, UTAH\n",
      "1260  10242430              GRASSY CREEK NEAR ENTERPRISE, UTAH\n",
      "1261  13077700                    GEORGE CREEK NEAR YOST, UTAH\n",
      "1262  13079000                     CLEAR CREEK NEAR NAF, IDAHO\n",
      "\n",
      "[666 rows x 2 columns]\n",
      "Data exported to usgs_utah_sites_with_sql_data.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Fetch USGS Sites\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    data = response.json()\n",
    "    sites = []\n",
    "    \n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        \n",
    "        # Only include site codes with 8 or fewer digits\n",
    "        if site_code.isdigit() and len(site_code) <= 8:\n",
    "            sites.append((site_code.strip(), site_name.strip()))\n",
    "\n",
    "    return pd.DataFrame(sites, columns=['Site Code', 'Site Name']).drop_duplicates(subset=['Site Code'])\n",
    "\n",
    "# Fetch SQL Data using SQLAlchemy\n",
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "    \n",
    "    conn_str = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    engine = create_engine(conn_str)\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName,\n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n",
    "        [LAT], [LON], [SiteState], [Status]\n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n",
    "        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    \"\"\"\n",
    "\n",
    "    df_sql = pd.read_sql(query, engine)\n",
    "    return df_sql\n",
    "\n",
    "# Load data\n",
    "df_usgs = get_usgs_sites()\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Prepare columns for merging (strip whitespaces, lower case, and convert to string)\n",
    "df_usgs['Site Code'] = df_usgs['Site Code'].astype(str).str.strip().str.lower()\n",
    "df_sql['CollectionStationName'] = df_sql['CollectionStationName'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Check unmatched site codes before merging\n",
    "unmatched = df_usgs[~df_usgs['Site Code'].isin(df_sql['CollectionStationName'])]\n",
    "if not unmatched.empty:\n",
    "    print(\"Unmatched USGS Site Codes:\")\n",
    "    print(unmatched[['Site Code', 'Site Name']])\n",
    "\n",
    "# Merge DataFrames\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites_with_sql_data.csv', index=False)\n",
    "print(\"Data exported to usgs_utah_sites_with_sql_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e68d171-4107-437c-928d-8a0a4c41f87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbenko\\AppData\\Local\\Temp\\1\\ipykernel_14876\\3674623515.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_sql = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "ename": "DatabaseError",
     "evalue": "Execution failed on sql '[YOUR SQL QUERY WITHOUT RECORD_YEAR]': ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Could not find stored procedure 'YOUR SQL QUERY WITHOUT RECORD_YEAR'. (2812) (SQLExecDirectW)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2674\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2674\u001b[0m     cur\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   2675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cur\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Could not find stored procedure 'YOUR SQL QUERY WITHOUT RECORD_YEAR'. (2812) (SQLExecDirectW)\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mDatabaseError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Fetch and prepare data\u001b[39;00m\n\u001b[0;32m     41\u001b[0m df_usgs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(get_usgs_sites(), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSite Code\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSite Name\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 42\u001b[0m df_sql \u001b[38;5;241m=\u001b[39m get_sql_data()\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Merge data on Site Code and CollectionStationName\u001b[39;00m\n\u001b[0;32m     45\u001b[0m df_merged \u001b[38;5;241m=\u001b[39m df_usgs\u001b[38;5;241m.\u001b[39mmerge(df_sql, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSite Code\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectionStationName\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 34\u001b[0m, in \u001b[0;36mget_sql_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m[YOUR SQL QUERY WITHOUT RECORD_YEAR]\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyodbc\u001b[38;5;241m.\u001b[39mconnect(\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDRIVER=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;124mODBC Driver 17 for SQL Server\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSERVER=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mserver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPWD=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     33\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m---> 34\u001b[0m     df_sql \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(query, conn)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Ensure CollectionStationName is zero-padded to 8 digits\u001b[39;00m\n\u001b[0;32m     37\u001b[0m df_sql[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectionStationName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_sql[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCollectionStationName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m8\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:706\u001b[0m, in \u001b[0;36mread_sql\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, columns, chunksize, dtype_backend, dtype)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pandas_sql, SQLiteDatabase):\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pandas_sql\u001b[38;5;241m.\u001b[39mread_query(\n\u001b[0;32m    707\u001b[0m             sql,\n\u001b[0;32m    708\u001b[0m             index_col\u001b[38;5;241m=\u001b[39mindex_col,\n\u001b[0;32m    709\u001b[0m             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    710\u001b[0m             coerce_float\u001b[38;5;241m=\u001b[39mcoerce_float,\n\u001b[0;32m    711\u001b[0m             parse_dates\u001b[38;5;241m=\u001b[39mparse_dates,\n\u001b[0;32m    712\u001b[0m             chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    713\u001b[0m             dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    714\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    715\u001b[0m         )\n\u001b[0;32m    717\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    718\u001b[0m         _is_table_name \u001b[38;5;241m=\u001b[39m pandas_sql\u001b[38;5;241m.\u001b[39mhas_table(sql)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2738\u001b[0m, in \u001b[0;36mSQLiteDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   2727\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2729\u001b[0m     sql,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2736\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2737\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[1;32m-> 2738\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(sql, params)\n\u001b[0;32m   2739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m [col_desc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m col_desc \u001b[38;5;129;01min\u001b[39;00m cursor\u001b[38;5;241m.\u001b[39mdescription]\n\u001b[0;32m   2741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\sql.py:2686\u001b[0m, in \u001b[0;36mSQLiteDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minner_exc\u001b[39;00m\n\u001b[0;32m   2685\u001b[0m ex \u001b[38;5;241m=\u001b[39m DatabaseError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecution failed on sql \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ex \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mDatabaseError\u001b[0m: Execution failed on sql '[YOUR SQL QUERY WITHOUT RECORD_YEAR]': ('42000', \"[42000] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Could not find stored procedure 'YOUR SQL QUERY WITHOUT RECORD_YEAR'. (2812) (SQLExecDirectW)\")"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    sites = set()\n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        if len(site_code) == 8:\n",
    "            sites.add((site_code, site_name))\n",
    "    return list(sites)\n",
    "\n",
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "    query = \"\"\"[YOUR SQL QUERY WITHOUT RECORD_YEAR]\"\"\"\n",
    "    \n",
    "    with pyodbc.connect(\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={server};\"\n",
    "        f\"DATABASE={database};\"\n",
    "        f\"UID={username};\"\n",
    "        f\"PWD={password}\"\n",
    "    ) as conn:\n",
    "        df_sql = pd.read_sql(query, conn)\n",
    "    \n",
    "    # Ensure CollectionStationName is zero-padded to 8 digits\n",
    "    df_sql['CollectionStationName'] = df_sql['CollectionStationName'].astype(str).str.zfill(8)\n",
    "    return df_sql\n",
    "\n",
    "# Fetch and prepare data\n",
    "df_usgs = pd.DataFrame(get_usgs_sites(), columns=['Site Code', 'Site Name'])\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Merge data on Site Code and CollectionStationName\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites.csv', index=False)\n",
    "\n",
    "print(\"Data exported to usgs_utah_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca58cb5-1d37-45b0-8c03-164720e8e1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pbenko\\AppData\\Local\\Temp\\1\\ipykernel_3852\\397204693.py:58: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_sql = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to usgs_utah_sites.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    sites = set()\n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        if len(site_code) == 8:\n",
    "            sites.add((site_code, site_name))\n",
    "    return list(sites)\n",
    "\n",
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], \n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n",
    "        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n",
    "        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n",
    "        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n",
    "        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n",
    "        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n",
    "        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n",
    "        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n",
    "        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n",
    "        [SeriesVerifiedDate], \n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n",
    "        STATION_MASTER.STATION_ID) AS StationPage\n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n",
    "        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n",
    "        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n",
    "        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    \"\"\"\n",
    "\n",
    "    with pyodbc.connect(\n",
    "        f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "        f\"SERVER={server};\"\n",
    "        f\"DATABASE={database};\"\n",
    "        f\"UID={username};\"\n",
    "        f\"PWD={password}\"\n",
    "    ) as conn:\n",
    "        df_sql = pd.read_sql(query, conn)\n",
    "    \n",
    "    # Ensure CollectionStationName is zero-padded to 8 digits\n",
    "    df_sql['CollectionStationName'] = df_sql['CollectionStationName'].astype(str).str.zfill(8)\n",
    "    return df_sql\n",
    "\n",
    "# Fetch and prepare data\n",
    "df_usgs = pd.DataFrame(get_usgs_sites(), columns=['Site Code', 'Site Name'])\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Merge data on Site Code and CollectionStationName\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites.csv', index=False)\n",
    "\n",
    "print(\"Data exported to usgs_utah_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21fa8bb9-ff7c-4c61-90e8-3fcefa0b7042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to usgs_utah_sites.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def get_usgs_sites(state_code='UT'):\n",
    "    url = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "    params = {'format': 'json', 'stateCd': state_code, 'siteStatus': 'all'}\n",
    "    response = requests.get(url, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    sites = set()\n",
    "    for site in data.get('value', {}).get('timeSeries', []):\n",
    "        site_info = site.get('sourceInfo', {})\n",
    "        site_name = site_info.get('siteName', 'Unknown')\n",
    "        site_code = site_info.get('siteCode', [{}])[0].get('value', 'Unknown')\n",
    "        if len(site_code) == 8:\n",
    "            sites.add((site_code, site_name))\n",
    "    return list(sites)\n",
    "\n",
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "    \n",
    "    # Create SQLAlchemy connection string\n",
    "    connection_string = (\n",
    "        f\"mssql+pyodbc://{username}:{password}@{server}/{database}\"\n",
    "        \"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    )\n",
    "    \n",
    "    # Create the SQLAlchemy engine\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        [COLLECTION_SYSTEM], [collection_sys_description], \n",
    "        [STATION_MASTER].[STATION_ID] AS MasterStationID,\n",
    "        [STATION_MASTER].[STATION_NAME] AS MasterStationName,\n",
    "        [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName, \n",
    "        [RETRIES], [SEQ_NO], [COMMENTS], [SiteType], [COMMON_DESC], \n",
    "        [DIVERTING_WORKS], [CAPTURE_SEQ_NO], [ANALOG_CHANNEL], \n",
    "        [STATION_ID], [SYSTEM_NAME], DatasetType, [MEASURING_DEVICE], \n",
    "        [DEVICE_TYPE], [OWNER_PHONE], [REALTIME_INCLUDE], [LAT], \n",
    "        [STATUS], [LON], [LOW_FLOW], [HIGH_FLOW], [SiteState], \n",
    "        [UNITS_DESC_BASE], CAPTURE_SEQ_NO, DataEntryMethod, \n",
    "        [Telemetry], [CORRECTED_DATA], [SeriesVerifiedBy], \n",
    "        [SeriesVerifiedDate], \n",
    "        CONCAT('https://waterrights.utah.gov/cgi-bin/dvrtview.exe?Modinfo=StationView&STATION_ID=', \n",
    "        STATION_MASTER.STATION_ID) AS StationPage\n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[COLLECTION_SYSTEMS] ON \n",
    "        [COLLECTION_SYSTEMS].[collection_sys_id] = [COLLECTION_STATIONS].[collection_sys_id]\n",
    "    LEFT JOIN [dvrtDB].[dbo].[STATION_MASTER] ON \n",
    "        [STATION_MASTER].[CAPTURE_SEQ_NO] = [COLLECTION_STATIONS].[SEQ_NO]\n",
    "    JOIN [dvrtDB].[dbo].[UNITS_MASTER] ON \n",
    "        [STATION_MASTER].[UNITS_ID] = [UNITS_MASTER].[UNITS_ID]\n",
    "    \"\"\"\n",
    "    \n",
    "    df_sql = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Ensure CollectionStationName is zero-padded to 8 digits\n",
    "    df_sql['CollectionStationName'] = df_sql['CollectionStationName'].astype(str).str.zfill(8)\n",
    "    return df_sql\n",
    "\n",
    "# Fetch and prepare data\n",
    "df_usgs = pd.DataFrame(get_usgs_sites(), columns=['Site Code', 'Site Name'])\n",
    "df_sql = get_sql_data()\n",
    "\n",
    "# Merge data on Site Code and CollectionStationName\n",
    "df_merged = df_usgs.merge(df_sql, left_on='Site Code', right_on='CollectionStationName', how='left')\n",
    "\n",
    "# Export to CSV\n",
    "df_merged.to_csv('usgs_utah_sites.csv', index=False)\n",
    "\n",
    "print(\"Data exported to usgs_utah_sites.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966b18be-ca8b-4bc2-b656-1dc6d72922c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      CollectionStationName\n",
      "0  10128500                                \n",
      "1  AAh                                     \n",
      "2  8768                                    \n",
      "3  Thayns Diversion                        \n",
      "4  Bear R. At Soda Spri                    \n"
     ]
    }
   ],
   "source": [
    "def get_sql_data():\n",
    "    server = 'wrt-sql-prod'\n",
    "    database = 'dvrtDB'\n",
    "    username = 'wrtsqlq'\n",
    "    password = 'guest'\n",
    "    \n",
    "    connection_string = (\n",
    "        f\"mssql+pyodbc://{username}:{password}@{server}/{database}\"\n",
    "        \"?driver=ODBC+Driver+17+for+SQL+Server\"\n",
    "    )\n",
    "    \n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT [COLLECTION_STATIONS].[STATION_NAME] AS CollectionStationName\n",
    "    FROM [dvrtDB].[dbo].[COLLECTION_STATIONS]\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df_sql = pd.read_sql(query, engine)\n",
    "        print(df_sql.head())  # Display the first few rows\n",
    "    except Exception as e:\n",
    "        print(\"Query failed:\", e)\n",
    "\n",
    "get_sql_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4066540-d7fc-4939-8c2c-0a1e3414a508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      COLLECTION_SYSTEM collection_sys_description  MasterStationID  \\\n",
      "0  LBEAR                        Bear River (Cache)             1721   \n",
      "1  LBEAR                        Bear River (Cache)             1753   \n",
      "2  LBEAR                        Bear River (Cache)             1728   \n",
      "3  LBEAR                        Bear River (Cache)             1725   \n",
      "4  LBEAR                        Bear River (Cache)             1777   \n",
      "\n",
      "                                   MasterStationName  \\\n",
      "0  22  LARRY FALSLEV                             ...   \n",
      "1  52  WAYNE GIBBS - WHEELER RANCH LLC           ...   \n",
      "2  29  JACKSON SMITH                             ...   \n",
      "3  26  WAYNE GIBBS - WHEELER RANCH               ...   \n",
      "4  T15 ALLEN                                     ...   \n",
      "\n",
      "                      CollectionStationName  RETRIES  SEQ_NO  \\\n",
      "0  AO                                             12       3   \n",
      "1  Ac                                             40       7   \n",
      "2  Ac                                             40       7   \n",
      "3  Ad                                             25       9   \n",
      "4  AAK                                            15      10   \n",
      "\n",
      "                                            COMMENTS    SiteType  \\\n",
      "0  22, Radio - 202 UP&L                          ...  Conveyance   \n",
      "1  29 & 52 -, Radio - 202 UP&L                   ...  Conveyance   \n",
      "2  29 & 52 -, Radio - 202 UP&L                   ...  Conveyance   \n",
      "3  26 -, Radio - 202 UP&L                        ...  Conveyance   \n",
      "4  T15  -, Radio - 206 Gravel Pit                ...  Conveyance   \n",
      "\n",
      "                                         COMMON_DESC  ... HIGH_FLOW  \\\n",
      "0  3500 West 3200 North - Benson                 ...  ...      2.00   \n",
      "1  12300 North 420 West - S.W. of Lewiston       ...  ...      2.30   \n",
      "2  12300 North 4200 West - S.W. of Lewiston      ...  ...      2.09   \n",
      "3  4000 West 12000 North S.W. of Lewiston        ...  ...      2.50   \n",
      "4  Cub River - E. of Lewiston                    ...  ...      1.50   \n",
      "\n",
      "   SiteState                           UNITS_DESC_BASE  CAPTURE_SEQ_NO  \\\n",
      "0         UT  Mean daily discharge in CFS                            3   \n",
      "1         UT  Mean daily discharge in CFS                            7   \n",
      "2         UT  Mean daily discharge in CFS                            7   \n",
      "3         UT  Mean daily discharge in CFS                            9   \n",
      "4         UT  Mean daily discharge in CFS                           10   \n",
      "\n",
      "  DataEntryMethod Telemetry CORRECTED_DATA SeriesVerifiedBy  \\\n",
      "0          Direct       Yes              Y             None   \n",
      "1          Direct       Yes              Y             None   \n",
      "2          Direct       Yes              Y             None   \n",
      "3          Direct       Yes              Y             None   \n",
      "4          Direct       Yes              Y             None   \n",
      "\n",
      "  SeriesVerifiedDate                                        StationPage  \n",
      "0               None  https://waterrights.utah.gov/cgi-bin/dvrtview....  \n",
      "1               None  https://waterrights.utah.gov/cgi-bin/dvrtview....  \n",
      "2               None  https://waterrights.utah.gov/cgi-bin/dvrtview....  \n",
      "3               None  https://waterrights.utah.gov/cgi-bin/dvrtview....  \n",
      "4               None  https://waterrights.utah.gov/cgi-bin/dvrtview....  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "df_sql['CollectionStationName'] = df_sql['CollectionStationName'].astype(str).str.zfill(8)\n",
    "print(df_sql.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da968828-f745-4bc2-9eab-31c6c9add35d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
